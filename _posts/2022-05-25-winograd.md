---
layout: post
title: "Winograd Schema Challenge"
subtitle: "Commonsense Reasoning in NLP"
background: '/img/posts/2022-05-25-winograd.jpg'
---

## Introduction

In preparation to attend the retirement of an world-reknwn Linguistics professor and her eminant students, all luminaries in the academic world, the neurotic me took this as a perfect excuse to dive deep into the world of NLP and learn what's happening in this exciting field. Today, I'll share an update of my learnings with you, particularly about the fascinating Winograd Schema Challenge.

## Reverse Timeline

Let's take a journey through time to understand the evolution of NLP models. In 2017, large language models, specifically Transformers, made a significant impact. Before that, in the 2010s, probabilistic models like neural networks, RNNs, GRUs, and LSTMs took the center stage. Moving further back, in the 1990s, statistical models utilizing n-gram co-occurrence were prevalent. And in the pre-90s era, rule-based systems ruled the domain.

> "You shall know a word by the company it keeps" - J.R. Firth, Linguist

This quote captures the essence of why statistical and probabilistic models work well when building language models. These models consider the nearby neighbors, syntax, rules of parts of speech, and semantic relationships within the context. However, it's essential to clarify that language models are not designed for true understanding. Instead, they focus on predicting the probability of the next word in a given sequence.

Encoder-Decoder Models
: Within the realm of NLP, there are different architectures for language models. The encoder-decoder models, also known as many-to-many (Seq2Seq), excel in tasks such as machine translation, text summarization, and question answering. Examples of these models include T5 and BART.

Encoder-Only Models
: On the other hand, encoder-only models employ many-to-one architecture. They shine in sentiment analysis and classification tasks. Prominent examples of encoder-only models are BERT, RoBERTa, and their numerous variants. These models also employ masked (cloze) language modeling techniques.

Decoder-Only Models
: Finally, decoder-only models utilize a one-to-many architecture, primarily focusing on text generation. GPT, GPT-2, and GPT-3 are renowned examples of these models. They employ causal (autoregressive) language modeling.

## Exponential Growth of Large Language Models

The exponential growth of large language models has been a topic of intrigue. It reminds me of "The Most Human Human" scenario that took place in Brighton, England, back in September 2009. Participants engaged in a series of instant-message chats, attempting to convince judges that they were human. While this anecdote showcases the challenge of distinguishing between humans and AI, it also demonstrates the increasing capabilities of language models.

## Winograd Schema Challenge

Now, let's delve deeper into the intriguing Winograd Schema Challenge. This unique evaluation method serves as a rigorous assessment of common sense reasoning and intelligence. The challenge focuses on resolving sentence pairs that differ by only one or two words, requiring knowledge and common-sense reasoning to arrive at the correct answer.

For example, consider the original Winograd Schema: "The town council members refused to give the angry demonstrators a permit because they feared/advocated violence." The question is, who feared/advocated violence? The answer could be either the town council members or the angry demonstrators, depending on the word choice.

The Winograd Schema Challenge consists of 273 pairs of sentences, each posing a similar knowledge and reasoning test. These challenges are designed to be easy for humans to understand but not solvable by simple techniques. They serve as a Google-proof evaluation, meaning there are no obvious statistical patterns that can lead to a correct answer.

One of the key advantages of the Winograd Schema Challenge is that it allows non-experts to determine whether a program fails or succeeds in resolving the sentence pairs. It offers a clearer assessment of a model's ability to comprehend common sense and reason logically.

## Pitfalls - The Quest for Precision

While the Winograd Schema Challenge presents an exciting opportunity to evaluate common sense reasoning, it is not without its challenges. One common pitfall is that the challenge can be too easy or too ambiguous, making it difficult to gauge the true capabilities of a model. To overcome this, benchmarks like GLUE and SuperGLUE have been introduced to provide more comprehensive evaluations.

Additionally, the evaluation criteria for the Winograd Schema Challenge can sometimes be too lax, and the creation of high-quality challenges manually poses its own set of difficulties. There may also be artifacts within the datasets that need to be addressed to ensure accurate assessments. Furthermore, leakage from large training data can inadvertently influence a model's performance, impacting the integrity of the evaluation.

## Surveying Related Datasets

To further explore the realm of common sense reasoning, several related datasets have been developed. These include the Winograd Natural Language Inference Dataset (WNLI) and SuperGLUE's WSC. Other datasets like Definite Pronoun Resolution (DPR), Pronoun Disambiguation Problem (PDP), WinoGender & WinoBias, WinoGrande, WinoFlexi, Winventor, WinoWhy & WinoLogic, and WinoMT & Wino-X expand the scope and depth of common sense evaluations. Moreover, these datasets have been extended to different languages such as French, Portuguese, Japanese, Chinese, Hindi, Slovene, and Hebrew.

## Continued Challenges

While the Winograd Schema Challenge has made significant strides in assessing common sense reasoning, there are ongoing challenges and opportunities for improvement. The initial dataset used for training and tuning has limitations, and the focus on solving pronoun resolution does not cover the broader scope of common sense reasoning or intelligence.

Addressing these challenges requires stricter evaluation criteria, meticulous dataset creation, and advancements in model training techniques. It is crucial to recognize that pronoun disambiguation alone does not provide a reliable measure of a model's overall grasp of commonsense knowledge or language understanding.

As researchers and developers continue to push the boundaries of NLP and AI, the quest for better common sense reasoning models persists. The Winograd Schema Challenge, along with related datasets and benchmarks, serves as a stepping stone towards achieving a deeper understanding of intelligence and the complexities of human-like reasoning in machines.

In conclusion, the Winograd Schema Challenge stands as a testament to the ever-evolving landscape of NLP. It prompts us to question, explore, and push the boundaries of AI.

- [slides](/docs/JournalClub%202022-05-25%20NLP.pdf)
