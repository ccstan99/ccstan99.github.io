---
layout: page
title: Resources
description: Learning about LLMs & AI safety
background: '/img/bg-about.jpg'
---

Thanks to the treasure trove of resources readily available on the internet, I've been catching up on the latest in Artificial Intelligence (AI), particularly Natural Language Processing (NLP). My aim is to concentrate on educational outreach, bringing STEM to underrepresented populations.

Since Large Language Models (LLMs) are all the rage now, I'm often asked, "Where should I start?" Below are some of my favorite to learn the basics. The first 3 from DeepLearning.AI on Coursera are each 1-hour overviews -- more have been released recently. While I haven't personally used the last two resources, LLM University & LLM Bootcamp, they come highly recommended for those seeking more comprehensive coverage.

## Large Language Models

- [ChatGPT Prompt Engineering for Developers](https://learn.deeplearning.ai/chatgpt-prompt-eng/)
- [Building Systems with the ChatGPT API](https://learn.deeplearning.ai/chatgpt-building-system/)
- [LangChain for LLM Application Development](https://learn.deeplearning.ai/langchain/)
- [Cohere's LLM University](https://docs.cohere.com/docs/llmu)
- [LLM Bootcamp](https://fullstackdeeplearning.com/llm-bootcamp/)

Otherwise, below are some resources that were particularly helpful in my own AI/ML learning.

## Deep Learning

- Coursera [DeepLearning](https://www.coursera.org/specializations/deep-learning) specialization
- Coursera [Natural Language Processing](https://www.coursera.org/specializations/natural-language-processing) specialization
- Stanford CS224n: [Natural Language Processing with Deep Learning](http://web.stanford.edu/class/cs224n/)
- [HuggingFace Course](https://huggingface.co/course)
- [Hands-on Machine Learning with Scikit-Learn and TensorFlow](https://github.com/ageron/handson-ml)
- fast.ai [Practical Deep Learning for Coders](https://course.fast.ai/)
- Edx [Google AI for JavaScript developers with Tensorflow.js](https://www.edx.org/course/google-ai-for-javascript-developers-with-tensorflowjs)
- AGI Safety Fundamentals [Technical Alignment Curriculum](https://www.eacambridge.org/technical-alignment-curriculum)

## NLP General Overview

- [The Unreasonable Effectiveness of Recurrent Neural Networks](http://karpathy.github.io/2015/05/21/rnn-effectiveness/) (2015)
- [Understanding LSTM Networks](https://colah.github.io/posts/2015-08-Understanding-LSTMs/) (2015)
- [Attention and Augmented RNNs](https://distill.pub/2016/augmented-rnns/)(2016)
- [The Annotated Transformer](http://nlp.seas.harvard.edu/2018/04/03/attention.html) (2018)
- [The Illustrated Transformer](https://jalammar.github.io/illustrated-transformer/) (2018)
- [The Illustrated BERT, ELMo, and co.](https://jalammar.github.io/illustrated-bert/) (2021)

## Journal Papers

- [A neural probabilistic language model](https://www.semanticscholar.org/paper/A-Neural-Probabilistic-Language-Model-Bengio-Ducharme/6c2b28f9354f667cd5bd07afc0471d8334430da7)
- Word2Vec
    + [Efficient estimation of word representations in vector space](https://arxiv.org/abs/1301.3781) (2013) \
    CBOW & Skip-Gram
    + [Distributed representation of words and phrases and their compositionality](https://arxiv.org/abs/1310.4546) (2013) \
    Negative Sampling
- [Show, attend and tell: Neural image caption generation with visual attention](https://arxiv.org/abs/1502.03044) (2015)
- Transformers: [Attention is All you Need](https://arxiv.org/abs/1706.03762) (2017)

My [notes](/2022/05/09/nlp-notes.html) include more definitions and links to other papers.
