---
layout: page
title: Resources
description: learning AI & NLP
background: '/img/bg-about.jpg'
---
Thanks to the incredible wealth of resources readily available on the internet, I've been catching up on the latest in Artificial Intelligence (AI), particularly Natural Language Processing (NLP). My hope is to focus on educational outreach, bringing STEM to underrepresented populations. Here's some that were particularly helpful in my own learning.

## Courses

- Coursera [DeepLearning](https://www.coursera.org/specializations/deep-learning) specialization 
- Coursera [Natural Language Processing](https://www.coursera.org/specializations/natural-language-processing) specialization
- Stanford CS224n: [Natural Language Processing with Deep Learning](http://web.stanford.edu/class/cs224n/)
- [HuggingFace Course](https://huggingface.co/course)
- Edx [Google AI for JavaScript developers with Tensorflow.js](https://www.edx.org/course/google-ai-for-javascript-developers-with-tensorflowjs)
- [Hands-on Machine Learning with Scikit-Learn and TensorFlow](https://github.com/ageron/handson-ml)
- fast.ai [Practical Deep Learning for Coders](https://course.fast.ai/)
- Animated Math [3Blue1Bown](https://www.3blue1brown.com/)
- AGI Safety Fundamentals [Technical Alignment Curriculum](https://www.eacambridge.org/technical-alignment-curriculum)

## NLP General Overview

- [The Unreasonable Effectiveness of Recurrent Neural Networks](http://karpathy.github.io/2015/05/21/rnn-effectiveness/) (2015)
- [Understanding LSTM Networks](https://colah.github.io/posts/2015-08-Understanding-LSTMs/) (2015)
- [Attention and Augmented RNNs](https://distill.pub/2016/augmented-rnns/)(2016)
- [The Annotated Transformer](http://nlp.seas.harvard.edu/2018/04/03/attention.html) (2018)
- [The Illustrated Transformer](https://jalammar.github.io/illustrated-transformer/) (2018)
- [The Illustrated BERT, ELMo, and co.](https://jalammar.github.io/illustrated-bert/) (2021)

## Journal Papers

- [A neural probabilistic language model](https://www.semanticscholar.org/paper/A-Neural-Probabilistic-Language-Model-Bengio-Ducharme/6c2b28f9354f667cd5bd07afc0471d8334430da7)
- Word2Vec
    + [Efficient estimation of word representations in vector space](https://arxiv.org/abs/1301.3781) (2013)
    <br/>CBOW & Skip-Gram
    + [Distributed representation of words and phrases and their compositionality](https://arxiv.org/abs/1310.4546) (2013)
    <br/>Negative Sampling
- [Show, attend and tell: Neural image caption generation with visual attention](https://arxiv.org/abs/1502.03044) (2015)
- Transformers: [Attention is All you Need](https://arxiv.org/abs/1706.03762) (2017)

My [notes](/2022/05/09/nlp-notes.html) include more definitions and links to other papers.
