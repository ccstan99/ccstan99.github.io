<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.2.2">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2023-05-30T01:17:14-07:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">ccstan99</title><subtitle>ccstan99&apos;s learning journal</subtitle><author><name>ccstan99</name></author><entry><title type="html">Demystifying ChatGPT + LLMs</title><link href="http://localhost:4000/2023/05/24/chatgpt-llms.html" rel="alternate" type="text/html" title="Demystifying ChatGPT + LLMs" /><published>2023-05-24T00:00:00-07:00</published><updated>2023-05-24T00:00:00-07:00</updated><id>http://localhost:4000/2023/05/24/chatgpt-llms</id><content type="html" xml:base="http://localhost:4000/2023/05/24/chatgpt-llms.html"><![CDATA[<h2 id="introduction">Introduction</h2>

<p>Welcome to the world of ChatGPT and LLMs filled with alphabet soup jargon. We’ll look at language models, how they got so large, and think about how to deal with all this AI buzz.</p>

<h2 id="background-in-natural-language-processing-nlp">Background in Natural Language Processing (NLP)</h2>

<p>Language models live in the field of natural language processing, The ultimate dream, the holy grail, is to talk with computers just like talking with any other person and have it completely understand you. No need to learn computer languages. They can speak ours. Originally, linguists would painstakingly create “expert” systems, hand-coding  grammar rules and all the vocabulary. Unfortunately, the end results were mediocre, at best.</p>

<p><img src="/img/posts/2023-05-24-llms-stats.png" width="80%" style="display: block; margin: 0 auto;" /></p>

<p>Towards the late 90s, statistical methods were used. There’s a famous quote, “You shall know a word by the company it keeps.” That simple idea was profound. If you start counting words and looking at what other words tend to occur with it, you’ll end up learning a lot about the language itself. All those relationships make up the language model. Think back to when you were first learning your native language. As a child, you were immersed with sounds, words, and sentences of your language. Now, you have intuition of what sounds right or what’s off, even if you don’t know why. You’ve internalized all the statistics for your own language model.</p>

<h2 id="rise-of-transformers">Rise of Transformers</h2>

<p>Transformers are a special kind of neural network introduced by Google in 2017.</p>

<p>Before that, language models were painfully slow and forgetful. Imagine a sweet old granny struggling to understand words one by one and getting confused by long sentences. So, if you had tried to make models bigger, trained on more data, it didn’t matter.</p>

<p><img src="/img/posts/2023-05-24-llms-superhero.png" width="80%" style="display: block; margin: 0 auto;" /></p>

<p>Things didn’t really improve much until Transformers came along with attention superpowers. Now, models could look at everything, all at once. Long-term memory? No problem. Now, we can train on all of Wikipedia and piles and piles of books. The models kept learning and improving. Turns out, neural networks are extremely data hungry. They need tons of it to learn. So people went out and scraped everything they could find on the web: blogs, tweets, code, whatever was out there.</p>

<p>Another innovation: Transformers could process data in parallel. Otherwise, it’s like one checkout stand serving 100 customers one at a time, sequentially. Instead, Transformers use 100 checkout stands to serve all 100 customers at the same time in parallel. So models got bigger, running faster, training longer on more data, and performance kept getting better!</p>

<p>Thus began the rise of large language models. Size is measured by the number of parameters or neurons. Today, to be large, a language model must have at least 1 billion neurons. They’re not the same, but for reference, the human brain has around 85 billion. And all the large language models today are some variant of Transformers, which completely revolutionized AI beyond just language!</p>

<h2 id="generative-pre-trained-transformers-gpt">Generative pre-trained transformers (GPT)</h2>

<p>Generative pre-trained transformers or GPT shine at generating text.</p>

<p>They are trained for next word prediction. It’s like a play the game. You grab a book, pick a sentence, and show the model the beginning of the sentence. It tries to guess the next word in the sentence. You show it the actual next word. Hmm, if it’s wrong, let’s update the neurons to do better the next time. Pick another sentence. Guess another word and update. Then repeat until the model gets really good at guessing the next words. Starting with “Twinkle Twinkle Little…” the next word is most likely “Star.” Maybe feels like a sophisticated auto-complete. Think about when you’re talking with someone and you “complete each other’s sentences.” It’s often a sign of deep mutual understanding. Imagine a computer doing the same.</p>

<p><img src="/img/posts/2023-05-24-llms-training.jpg" width="80%" style="display: block; margin: 0 auto;" /></p>

<p>When it comes to P in Pre-training we need to look to the past. A variety of different models would be trained as specialists: one for answering questions, another for classifying things, or for translation. But GPT is a generalist, pre-trained to understand human languages in general, which can be quickly fine-tuned on top of the pre-trained base.</p>

<p>GPT was created by OpenAI. The original, GPT-1, was introduced in 2018, jumping in after Google’s transformers. The next year, GPT-2 was 10 times larger. The year after that, GPT-3 was 100 times larger. By then, it was almost impossible to tell what’s generated and what’s not. As for GPT-4 that just came out? It’s a secret. Keep in mind, these models are huge. It costs millions of dollars to train on a slew of computers over a period of months. It’s not feasible for individuals or even academic researchers to train anything like this. You need those deep pockets.</p>

<h2 id="chatgpt">ChatGPT</h2>

<p>Then along came ChatGPT. Frankly, many working in the field were baffled, “Why is this going viral now? It’s just GPT-3.5, with minor tweaks to the base model released three years ago!”</p>

<p>The secret sauce was reinforcement learning from human feedback. Remember, GPT was trained to predict the next word given the start of a sentence. That’s great for learning the language, but really, that’s not as helpful for the typical end user. When you talk to people, you rarely begin a sentence and then expect them to complete it. Instead, you’re more likely to tell them to do something or maybe ask a question.</p>

<p>So GPT-3 was fine-tuned to respond to instructions. They got lots of sample instructions along with what’s expected. If you see “Prepare a report,” the output should indicate what a report looks like. For “Give me a summary,” the output should indicate what a summary looks like.</p>

<p><img src="/img/posts/2023-05-24-llms-instruct.jpg" width="80%" style="display: block; margin: 0 auto;" /></p>

<p>Next, it was also fine-tuned on sample back-and-forth conversations to learn how dialogues work. These extra fine-tuning steps were quick and cheap compared to pre-training the original, which took months and millions. It managed to bring out some innate abilities hidden in the base model. But suddenly, following instructions and carrying conversations made it extremely helpful and eerily “human.”</p>

<h2 id="on-the-horizon">On the Horizon</h2>

<p>That us up to late last year. Now, things are moving even faster since then. Let me point out a few items on the horizon to keep an eye on.</p>

<dl>
  <dt>Multi-modality</dt>
  <dd>Language models deal mainly with text. Multi-modality allows them to use audio, images, and videos to understand the world. And the better to see you, to hear you, and to interact with you.</dd>
  <dt>Open-Source</dt>
  <dd>Just a few weeks ago, Meta released an open-sourced large language model that can run on personal computers. That led to a flurry of activity. Researchers are now creating much smaller and cheaper models that are still powerful.</dd>
  <dt>Agents</dt>
  <dd>Language models can generate text for a plan. Now, they can also call agents, which are programs to execute that plan. “Plan a trip to Tokyo.” It goes to compare prices, book flights, find lodging. You just show up and enjoy. But if AI automatically connects to agents that can change things in the real world with consequences, we must stop and think, “What could possibly go wrong?!”</dd>
</dl>

<h2 id="hype-or-reality">Hype or Reality?</h2>

<p>Is this all hype? Or is the world really changing? I started working in tech during the dot-com era. The gold rush euphoria feels suspiciously familiar. But there’s definitely a fundamental paradigm shift that’s both thrilling but a tad worrying.</p>

<p>Personally, I use large language models every day. They’re great for mundane tasks like proofreading documents, drafting emails, or generating code snippets. Search is really handy. Information is synthesized and summarized for your specific case. But I only use AI as a tool to speed up my normal workflow. I still double-check the output since it’s not all sunshine and rainbows.</p>

<p>You must understand the limitations. Hallucinations are a big problem. That’s a technical term when the output is just flat out wrong but sounds really convincing and confident. A dangerous combination. Essentially, AI is a master B.S.er, but plenty of humans are too. It’s just regurgitating what’s learned from its training data, which includes lots of toxic and biased stuff too. So what’s the key takeaway?</p>

<h2 id="conclusion">Conclusion</h2>

<p>We’re standing at the dawn of this new era shaped by AI. In order to survive, you’ll have to embrace AI in some way. No one wants to be the dinosaur. Sometimes, it feels like we’re blindly rushing into this awe-inspiring yet potentially treacherous territory. AI is a tool that holds immense power, but it’s far from perfect or even safe. As the saying goes, “Trust but verify.” We need to keep humans in the loop. My hope is that we can be guided by wisdom, empathy, and a commitment to embrace the power of AI responsibly.</p>

<p><img src="/img/posts/2023-05-24-llms-embrace.png" width="80%" style="display: block; margin: 0 auto;" /></p>]]></content><author><name>ccstan99</name></author><summary type="html"><![CDATA[Introduction]]></summary></entry><entry><title type="html">Google I/O 2023</title><link href="http://localhost:4000/2023/05/10/io.html" rel="alternate" type="text/html" title="Google I/O 2023" /><published>2023-05-10T00:00:00-07:00</published><updated>2023-05-10T00:00:00-07:00</updated><id>http://localhost:4000/2023/05/10/io</id><content type="html" xml:base="http://localhost:4000/2023/05/10/io.html"><![CDATA[<p>I had a memorable experience at <a href="https://io.google/2023/">Google I/O 2023</a>!</p>

<h2 id="pre-reception-in-santa-clara">Pre-Reception in Santa Clara</h2>

<p>The event kicked off with the Women Techmakers Ambassadors at the pre-reception on Tuesday evening. It was such an exciting moment to finally meet and connect with Creatives in Tech in person. We were buzzing with anticipation, eager to learn from each other and soak up all the knowledge and inspiration that awaited us.</p>

<p><img src="/img/posts/2023-05-10-io-reception.jpg" width="80%" style="display: block; margin: 0 auto;" /></p>

<h2 id="keynote-at-shoreline">Keynote at Shoreline</h2>

<p>One of the highlights of the event was undoubtedly the I/O keynote addresses. It was wonderful to witness Google’s announcements firsthand and watch the impressive AI/ML demos. The entire event was incredibly well-organized, right from the convenient shuttle services to the delicious breakfast and lunch options. And to our surprise, we were even provided with caps, sweatshirts, sunscreen, rain ponchos, and hand warmers - they truly had us covered for any unexpected weather or situation.</p>

<p><img src="/img/posts/2023-05-10-io-keynote.jpg" width="80%" style="display: block; margin: 0 auto;" /></p>

<h2 id="breakouts-at-bay-view">Breakouts at Bay View</h2>

<p>After the keynote, we had the chance to attend breakout sessions at Google’s new Bayview campus. It was an opportunity to delve deeper into topics like women in leadership and AI/ML. The sessions were informative, engaging, and filled with valuable insights. We couldn’t resist exploring the beautiful campus and taking advantage of the bike trails. It was a refreshing way to unwind and enjoy the surroundings.</p>

<p><img src="/img/posts/2023-05-10-io-bayview.jpg" width="80%" style="display: block; margin: 0 auto;" /></p>

<p>I had such a fantastic time at Google I/O 2023 and highly recommend attending the event in the future. The atmosphere was fun-filled, the knowledge-sharing was inspiring, and the connections made were invaluable. Can’t wait for next time!</p>

<p>Until then, keep exploring, learning, and embracing the exciting world of technology!</p>]]></content><author><name>ccstan99</name></author><summary type="html"><![CDATA[I had a memorable experience at Google I/O 2023!]]></summary></entry><entry><title type="html">Dare to be Creative in Tech</title><link href="http://localhost:4000/2023/03/22/wtm-iwd.html" rel="alternate" type="text/html" title="Dare to be Creative in Tech" /><published>2023-03-22T00:00:00-07:00</published><updated>2023-03-22T00:00:00-07:00</updated><id>http://localhost:4000/2023/03/22/wtm-iwd</id><content type="html" xml:base="http://localhost:4000/2023/03/22/wtm-iwd.html"><![CDATA[<p>Are you a woman in tech? Do you want to hear from other creative and talented women in the industry? Look no further than the Women Techmakers Ambassador community, Creatives in Tech! On March 22nd, we celebrated International Women’s Day by hosting a virtual event that highlighted the experiences of creative women in tech.</p>

<p><img src="/img/posts/2023-03-22-iwd-promo.jpg" width="80%" style="display: block; margin: 0 auto;" /></p>

<h2 id="our-event">Our Event</h2>

<p>The 6-hour event featured keynote speeches, panel discussions, and even a job board filled with potential job opportunities for creative tech roles.</p>

<dl>
  <dt>Opening: Dare to Bring Diversity to Web 3.0</dt>
  <dd>(10 am PDT / 12pm CDT / 1pm EDT)</dd>
  <dd>Keynote Speaker: Tina Bonner, CEO &amp; Founder Black in Meta</dd>
  <dt>Embracing Creativity in Tech</dt>
  <dd>(11 am PDT / 1pm CDT / 2pm EDT)</dd>
  <dd>Panelists: Kavitha Bangalore and Raj Sree</dd>
  <dd>Moderator: ChengCheng Tan</dd>
  <dt>Break and Job Jam!</dt>
  <dd>(12pm PDT / 2pm CDT / 3pm EDT)</dd>
  <dt>Creative Tech Corner</dt>
  <dd>(1pm PDT / 3pm CDT / 4pm EDT)</dd>
  <dd>Speakers: Vaishnavi Venkata Subramaniam and Ashley Clayton</dd>
  <dt>The Daring Mindset: Design Thinking</dt>
  <dd>(2pm PDT / 4pm CDT / 5pm EDT)</dd>
  <dd>Panelists: Kaylyn Hill and Michelle Ng</dd>
  <dd>Moderator: ChengCheng Tan</dd>
  <dt>How Daring to be Non-Traditional Secured Me a Job</dt>
  <dd>(3pm PDT / 5pm CDT / 6pm EDT)</dd>
  <dd>Keynote Speaker: Alaa Shahin, UX Designer at Univ of Michigan</dd>
</dl>

<p>We had an amazing turnout, with over 140 registrations on <a href="https://www.eventbrite.com/e/dare-to-be-creative-in-tech-tickets-560933706817">EventBrite</a>. <a href="https://gatheround.com/">Gatheround</a>, as our platform, was perfect for adding a creative touch and allowing for an interactive experience.</p>

<h2 id="our-team">Our Team</h2>

<p>It was an immense honor to befriend so many smart and talented Women Techmaker Ambassadors. Our team of 12 volunteers and 9 speakers worked together to create a comprehensive plan that included a general run of show and weekly meetings starting in mid-January. Kaylyn spearheaded the event, guided us withe her vision, and provided extensive planning documents to keep everything on track. Michelle, our wonderful head of marketing, put together a social media toolkit and recruited backend volunteers. I offered input from the early stages and became more active as an organizer towards the end of February, taking time off work to ensure that everything went smoothly. All the volunteers and speakers actively contributed ideas and ultimately formed a sisterly bond.</p>

<p><img src="/img/posts/2023-03-22-iwd-photobooth.png" width="80%" style="display: block; margin: 0 auto;" /></p>

<h2 id="join-our-community">Join Our Community</h2>

<p>Overall, it was an incredibly experience and we hope to continue building our community of talented women in tech. If you’re interested in joining us or attending future events, be sure to follow us on <a href="https://www.linkedin.com/company/creatives-in-tech/">LinkedIn</a> and keep an eye out for our next event announcement. We hope to see you there! ⚡️</p>

<p>This event was generously supported by <a href="https://developers.google.com/womentechmakers">Google Women Techmakers</a>, <a href="https://www.mylivestack.com/">WeTransact.Live</a>, and <a href="https://gatheround.com/">Gatheround</a>.</p>]]></content><author><name>ccstan99</name></author><summary type="html"><![CDATA[Are you a woman in tech? Do you want to hear from other creative and talented women in the industry? Look no further than the Women Techmakers Ambassador community, Creatives in Tech! On March 22nd, we celebrated International Women’s Day by hosting a virtual event that highlighted the experiences of creative women in tech.]]></summary></entry><entry><title type="html">Go Attack</title><link href="http://localhost:4000/2022/12/01/goattack-poster.html" rel="alternate" type="text/html" title="Go Attack" /><published>2022-12-01T00:00:00-08:00</published><updated>2022-12-01T00:00:00-08:00</updated><id>http://localhost:4000/2022/12/01/goattack-poster</id><content type="html" xml:base="http://localhost:4000/2022/12/01/goattack-poster.html"><![CDATA[<p>I am thrilled to share my recent helping FAR.AI, an AI Alignment research organization, creating the NeurIPS poster for the project <a href="https://goattack.far.ai/">“Adversarial Policies Beat Superhuman Go AIs.”</a></p>

<h2 id="research-overview">Research Overview</h2>

<p>The project, fondly nicknamed “Go Attack”, aimed to challenge the state-of-the-art Go-playing AI system KataGo. Researchers developed adversarial policies designed to play against KataGo. The adversarial policies achieved an astounding win rate of over 99% when KataGo used no tree search, and over 97% when it utilized enough search to be considered superhuman.</p>

<p>What’s even more surprising is that the adversaries did not win by playing Go better than KataGo. In fact, they were easily beaten by human amateurs. Instead, the adversaries won by exploiting weaknesses in KataGo, tricking it into making serious blunders. This attack strategy transferred effortlessly to other superhuman Go-playing AIs, showcasing the surprising failure modes even in the most advanced AI systems.</p>

<h2 id="reworked-figures">Reworked Figures</h2>

<p>To better communicate the research findings, I reworked images from previous presentations and figures, particularly focusing on clarifying the <strong>threat model</strong> and the concept of <strong>non-transitivity</strong> with the unexpected result: If play were transitive, Human players would not beat Adversary! Finally, I incorporated images generated from different Go board configurations to showcase various strategies we employed during testing.</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center">after</th>
      <th style="text-align: center">before</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center"><img src="/img/posts/2022-12-01-goattack-model.svg" width="400px" /><br />Threat Model</td>
      <td style="text-align: center"><img src="/img/posts/2022-12-01-goattack-model-before.png" width="400px" /></td>
    </tr>
    <tr>
      <td style="text-align: center"><img src="/img/posts/2022-12-01-goattack-nontransitivity.svg" width="400px" /><br />Non-Transitivity</td>
      <td style="text-align: center"><img src="/img/posts/2022-12-01-goattack-nontransitivity-before.png" width="400px" /></td>
    </tr>
  </tbody>
</table>

<h2 id="neurips-poster">NeurIPS Poster</h2>

<p>The various figures were crafted into narrative summaring the research journey, highlighting the significant achievements and implications of the findings. In addition to creating the digital poster, I also designed a template for the organization that will be used for future conference presentations.</p>

<p><img src="/img/posts/2022-12-01-goattack.png" width="100%" style="border:1px solid #eee; display: block; margin: 0 auto;" /></p>

<h2 id="conclusion">Conclusion</h2>

<p>It was an honor to make a small contribution to the field of AI safety. Learn more about the project by visiting the <a href="https://goattack.far.ai/">website</a>, reading the <a href="https://arxiv.org/abs/2211.00241">paper</a>, or exploring the <a href="https://github.com/AlignmentResearch/go_attack">code</a>. Special thansks to Adam Gleave for inviting me to help with the project and the many rounds of constructive feedbacks. Thanks also goes to Tony Wang who pointed out that the human figure resembled a cop and thug, which led to strengthening other visual cues such as changing the clothing color and adjusting the direction. Congrats to the team for winning a best paper award and an x-risk analysis award for the NeurIPS 2022 ML Safety Workshop!</p>]]></content><author><name>ccstan99</name></author><summary type="html"><![CDATA[I am thrilled to share my recent helping FAR.AI, an AI Alignment research organization, creating the NeurIPS poster for the project “Adversarial Policies Beat Superhuman Go AIs.”]]></summary></entry><entry><title type="html">Optimal Behavior Prior</title><link href="http://localhost:4000/2022/11/30/obp-poster.html" rel="alternate" type="text/html" title="Optimal Behavior Prior" /><published>2022-11-30T00:00:00-08:00</published><updated>2022-11-30T00:00:00-08:00</updated><id>http://localhost:4000/2022/11/30/obp-poster</id><content type="html" xml:base="http://localhost:4000/2022/11/30/obp-poster.html"><![CDATA[<h2 id="effective-use-of-human-data">Effective Use of Human Data</h2>
<p>Happy Thanksgiving! Would you have liked an AI preparing your feast? Or maybe cleaning up afterwards? On the road to creating collaborative AI agents that can better understand and assist humans, <a href="https://arxiv.org/abs/2211.01602">Optimal Behavior Prior</a> presents an efficient way to use data collected from humans using the <a href="https://github.com/HumanCompatibleAI/overcooked_ai">Overcooked-AI</a> benchmark environment.</p>

<h2 id="figure-rework">Figure Rework</h2>
<p>Through <a href="http://www.AlignmentFund.org">FAR (Fund for Alignment Research)</a>, I had the opportunity to design communications for the paper. The main figure was a centerpiece highlighting the key concepts from the paper and an algorithm also clarified the process.</p>

<p><img src="/img/posts/2022-11-30-obp-1a.png" width="80%" style="display: block; margin: 0 auto;" /></p>

<p><img src="/img/posts/2022-11-30-obp-algorithm.png" width="80%" style="display: block; margin: 0 auto;" /></p>

<p>While the original figure was already well-done, we explored other ways to strengthen the ideas by:</p>

<ul>
  <li>Stacking the environments to show a given environment with agents in greater detail.</li>
  <li>Combining the policy, agent, and neural network model images together into a single image emphasized the color &amp; labels more clearly.</li>
  <li>Simplifying the human-data image.</li>
  <li>Adding a narrative storyline with bold keywords to highlight the discrete steps.</li>
  <li>Incorporating subtext from the algorithm underneath keywords added to the explanation.</li>
</ul>

<p><img src="/img/posts/2022-11-30-obp-1b.png" width="80%" style="display: block; margin: 0 auto;" /></p>

<h2 id="poster-design">Poster Design</h2>
<p>After some additional back &amp; forth to finalize text description and LaTex labels, the newly redesigned figure and other key elements from the paper were extracted to create the resulting poster. Normally, I use Adobe Illustrator or InDesign for layout projects. However, in this case, figures were formatted in Figma then printed on a 2’x3’ poster.</p>

<p><img src="/img/posts/2022-11-30-obp-poster.png" width="80%" style="display: block; margin: 0 auto;" /></p>

<p>Check out the full paper, <a href="https://arxiv.org/abs/2211.01602">“Optimal Behavior Prior: Data-Efficient Human Models for Improved Human-AI Collaboration”</a> presented at the NeurIPS 2022 Human in the Loop Learning (HiLL) Workshop. Special thanks to Micah Carroll for providing the initial figures, sample poster templates, and constructive feedback throughout the entire process.</p>]]></content><author><name>ccstan99</name></author><summary type="html"><![CDATA[Effective Use of Human Data Happy Thanksgiving! Would you have liked an AI preparing your feast? Or maybe cleaning up afterwards? On the road to creating collaborative AI agents that can better understand and assist humans, Optimal Behavior Prior presents an efficient way to use data collected from humans using the Overcooked-AI benchmark environment.]]></summary></entry><entry><title type="html">Minimalist Logo</title><link href="http://localhost:4000/2022/11/20/stampy-min-logo.html" rel="alternate" type="text/html" title="Minimalist Logo" /><published>2022-11-20T00:00:00-08:00</published><updated>2022-11-20T00:00:00-08:00</updated><id>http://localhost:4000/2022/11/20/stampy-min-logo</id><content type="html" xml:base="http://localhost:4000/2022/11/20/stampy-min-logo.html"><![CDATA[<h2 id="fun-vs-minimal">Fun vs Minimal</h2>
<p>While some people loved the new Stampy logo, others felt it was a bit too “cutesy” and inappropriate to represent a serious topic as AGI safety and existential risk to humanity. So, an alternate website was launched with the same informational text content as stampy.ai but without the imagery. Between the various sites, the analytics indicated a healthy average of a few thousand visitors each week. We began exploring a more minimalistic logo for these alternate sites.</p>

<h2 id="generated-alignment">Generated Alignment</h2>
<p>With the idea of “alignment” and “seriousness” in mind, we first used namecheap’s <a href="https://www.namecheap.com/logo-maker/app/">Logo Maker</a> to generate some basic ideas. After answering a few questions on our preferences for font and logo styles &amp; colors, we gravitated towards indigo-bluish colors, some gradients, and parallel lines to indicate alignment.
<img src="/img/posts/2022-11-20-stampy-min-ideas1.png" width="50%" style="display: block; margin: 0 auto;" /></p>

<h2 id="puzzling-challenges">Puzzling Challenges</h2>
<p>For the next iteration, we brought in some additional ideas of mazes or puzzles, hinting at the idea of solving challenging problems. Someone suggested using concentric circles or colors from branding within the <a href="https://www.centreforeffectivealtruism.org/">Effective Altruism</a> movement.</p>

<p><img src="/img/posts/2022-11-20-stampy-min-ideas2.jpg" width="50%" style="display: block; margin: 0 auto;" /></p>

<p><img src="/img/posts/2022-11-20-stampy-min-ideas3.png" width="50%" style="display: block; margin: 0 auto;" /></p>

<h2 id="easter-eggs">Easter Eggs</h2>
<p>In our final rounds, we experimented with some Easter eggs, incorporating aspects of the original Stampy colors including elements of a face and stamp.</p>

<p><img src="/img/posts/2022-11-20-stampy-min-ideas4.png" width="50%" style="display: block; margin: 0 auto;" /></p>

<p>Finally, introducing our new alternate, minimalistic, sorta-serious logo… It’s a puzzle. It’s a maze. It’s a stamp. It’s face. It’s anything it wants to be! Do you see all the possibilities?</p>

<p><img src="/img/posts/2022-11-20-stampy-min-logo.svg" width="30%" style="display: block; margin: 0 auto;" /></p>

<p>Take a look at <a href="http://alignment.wiki">alignment.wiki</a> and <a href="https://aisafety.info/">aisafety.info</a> with the new, minimalistic design.</p>]]></content><author><name>ccstan99</name></author><summary type="html"><![CDATA[Fun vs Minimal While some people loved the new Stampy logo, others felt it was a bit too “cutesy” and inappropriate to represent a serious topic as AGI safety and existential risk to humanity. So, an alternate website was launched with the same informational text content as stampy.ai but without the imagery. Between the various sites, the analytics indicated a healthy average of a few thousand visitors each week. We began exploring a more minimalistic logo for these alternate sites.]]></summary></entry><entry><title type="html">AI Alignment Literature</title><link href="http://localhost:4000/2022/09/22/align-lit.html" rel="alternate" type="text/html" title="AI Alignment Literature" /><published>2022-09-22T00:00:00-07:00</published><updated>2022-09-22T00:00:00-07:00</updated><id>http://localhost:4000/2022/09/22/align-lit</id><content type="html" xml:base="http://localhost:4000/2022/09/22/align-lit.html"><![CDATA[<h2 id="alignment-literature-search">Alignment Literature Search</h2>

<p>The field of AI Safety &amp; Alignment is a new and very fluid field of study. There’s been an influx of recent research which makes it a daunting task for those interested in entering the field to get a good overview of the landscape and directions.</p>

<p>For the last few months, getting backing into web development for the Stampy UI and working on the semantic search have been rewarding. Since we functioned well together as a team, a number of software engineers have decided to turn full-time on AGI Safety &amp; Alignment to form the <a href="https://alignment.dev/">Alignment Ecosystem Development</a> group. I’m really excited to start on a new project with them, taking the <a href="https://github.com/moirage/alignment-research-dataset">Alignment Research Dataset</a> to be searchable for AI alignment researchers. This is my excuse er chance to get better acquainted with research literature while also working with some NLP techniques. In the meantime, I’ve also been attending a weekly to biweekly <a href="https://www.agisafetyfundamentals.com/ai-alignment-curriculum">AGI Safety Fundamentals</a> reading group.</p>

<p>Between the reading group and researching for the next project, I’m rapidly drowning in my random piles of handwritten notes. For now, I’ll be using various parts of this site and specificially this page to organize meandering thoughts and track progress, which will likely remain in a state of flux for some time.</p>

<h2 id="references">References</h2>

<ul>
  <li><a href="https://arxiv.org/abs/2206.02841">Understanding AI Alignment Research: A Systematic Analysis</a> <a href="https://github.com/moirage/alignment-research-dataset">[GitHub]</a> (2022)</li>
  <li><a href="https://arxiv.org/abs/2004.07180">SPECTER: Document-level Representation Learning using Citation-informed Transformers</a> <a href="https://github.com/allenai/specter">[GitHub]</a> (2020)</li>
  <li><a href="https://arxiv.org/abs/2004.15011">TLDR: Extreme Summarization of Scientific Documents</a> <a href="https://github.com/allenai/scitldr">[GitHub]</a> (2020)</li>
  <li><a href="https://arxiv.org/abs/1903.10676v3">SCIBERT: A Pretrained Language Model for Scientific Text</a> <a href="https://github.com/allenai/scibert">[GitHub]</a> (2019)</li>
  <li><a href="https://arxiv.org/abs/1903.10676v3">S2ORC: The Semantic Scholar Open Research Corpus</a> <a href="https://github.com/allenai/s2orc">(GitHub)</a> (2019)</li>
  <li><a href="https://arxiv.org/abs/1805.02262">Construction of the Literature Graph in Semantic Scholar</a> (2018)</li>
</ul>

<h2 id="open-domain-qa">Open Domain Q&amp;A</h2>

<ul>
  <li><a href="https://aclanthology.org/2020.acl-tutorials.8/">ACL2020 Tutorial: Open-Domain Question Answering</a> <a href="https://github.com/danqi/acl2020-openqa-tutorial">[GitHub]</a> (2020)</li>
  <li><a href="https://arxiv.org/abs/2209.10063">Generate rather than Retrieve: Large Language Models are Strong Context Generators</a></li>
  <li><a href="https://arxiv.org/abs/2009.08553">Generation-Augmented Retrieval for Open-domain Question Answering</a></li>
  <li><a href="https://arxiv.org/abs/2004.04906">Dense Passage Retrieval for Open-Domain Question Answering</a></li>
  <li><a href="https://arxiv.org/abs/1704.00051">Reading Wikipedia to Answer Open-Domain Questions</a></li>
</ul>

<h2 id="vector-databases-and-building-ml-apps">Vector Databases and Building ML Apps</h2>

<ul>
  <li><a href="https://milvus.io/">milvus</a> <a href="https://github.com/milvus-io/bootcamp">[GitHub]</a></li>
  <li><a href="https://www.pinecone.io/learn/">pinecone</a> <a href="https://github.com/pinecone-io">[GitHub]</a></li>
  <li><a href="https://www.gradio.app/getting_started/">gradio</a> <a href="https://github.com/gradio-app">[GitHub]</a></li>
  <li><a href="https://docs.streamlit.io/library/get-started">streamlit</a> <a href="https://github.com/streamlit">[GitHub]</a></li>
</ul>]]></content><author><name>ccstan99</name></author><summary type="html"><![CDATA[Alignment Literature Search]]></summary></entry><entry><title type="html">AI4ALL</title><link href="http://localhost:4000/2022/08/11/ai4all.html" rel="alternate" type="text/html" title="AI4ALL" /><published>2022-08-11T00:00:00-07:00</published><updated>2022-08-11T00:00:00-07:00</updated><id>http://localhost:4000/2022/08/11/ai4all</id><content type="html" xml:base="http://localhost:4000/2022/08/11/ai4all.html"><![CDATA[<h2 id="ai4all">AI4ALL</h2>

<p><a href="https://ai-4-all.org/">AI4ALL</a> opens doors to Artificial Intelligence for historically excluded talent through education and mentorship. They conduct <a href="https://ai-4-all.org/summer-programs/">summer programs</a> at universities across the US and offer extensive support to the alumni through their <a href="https://ai-4-all.org/changemakers/">Changemakers in AI</a> program.</p>

<p><img src="/img/posts/2022-08-11-ai4all1.png" width="100%" style="border:1px solid #eee" /></p>

<h2 id="summer-program">Summer Program</h2>

<p>The AI4ALL summer programs first entered our radar for my daughter, who had a long-time interest in AI. She attended the 3-week camp at Princeton in 2019 and had a blast learning about different ML algorithms &amp; AI ethics, conducting a hands-on research project using genomic data, and touring Washington DC to meet AI policy makers. The alumni network was even more impressive, with frequent updates on opportunities for internships and continued education.</p>

<h2 id="portfolio-project">Portfolio Project</h2>

<p>Soon, I found myself following along, learning these new concepts through their <a href="https://ai-4-all.org/open-learning/">Open Learning</a> platform for educators, then eventually studying deep learning and NLP on my own. A few months ago, I received a volunteer request for their 8-week long summer portfolio project. Students who already completed the regular summer program are given the opportunity to apply and deepen their knowledge of AI by working on an extended personal project. They attend weekly workshops, learning to select and pre-process their dataset, train and evaluate a model, then deliver a poster presentation of their findings.</p>

<p>Students were divided into 3 areas of interest: numeric, text, and images then paired with mentors having relevant experience. My mentees worked on natural language processing tasks, using BERT to classify text. I created a couple videos below and also wrote a <a href="https://colab.research.google.com/github/ccstan99/ccstan99.github.io/blob/main/docs/huggingface-text-classification.ipynb">tutorial notebook</a> to help get them started.</p>

<p><a href="https://www.youtube.com/watch?v=Q3N7zoIcjtw&amp;list=PLSGYwl5_qS6jEhXHXuEymvNYvrFuD2BOG&amp;index=1"><img src="/img/posts/2022-08-11-ai4all-video1.svg" width="45%" /></a>
<a href="https://www.youtube.com/watch?v=bedJ9bQBG6s&amp;list=PLSGYwl5_qS6jEhXHXuEymvNYvrFuD2BOG&amp;index=2"><img src="/img/posts/2022-08-11-ai4all-video2.svg" width="45%" /></a></p>

<p>Since AI4ALL has a rigorous applicant screening process, all the participants were very bright and motivated, which led to a very reward mentoring experience.</p>

<h2 id="conclusion">Conclusion</h2>

<p>I can fully attest to “teaching is the best way to learn.” While homeschooling my kids, I was constantly learning new material and always thinking, “How would I explain this?” That mentality forced me to think more deeply and analytically. As the kids are off in more traditional educational settings now, it’s been a natural transition for me to return to my work in the technology sector and also mentor other students. Mentoring has been an excellent way to supercharge my upskilling. I previously wrote about my experience with <a href="/2022/06/23/technovation.html">Technovations</a>.</p>

<p>I’d love to learn more about the many other wonderful programs out there. If you haven’t had a chance to share your skills through mentoring, definitely give it a try!</p>]]></content><author><name>ccstan99</name></author><summary type="html"><![CDATA[AI4ALL]]></summary></entry><entry><title type="html">Semantic Search in Browser</title><link href="http://localhost:4000/2022/07/05/use.html" rel="alternate" type="text/html" title="Semantic Search in Browser" /><published>2022-07-05T00:00:00-07:00</published><updated>2022-07-05T00:00:00-07:00</updated><id>http://localhost:4000/2022/07/05/use</id><content type="html" xml:base="http://localhost:4000/2022/07/05/use.html"><![CDATA[<h2 id="semantic-search">Semantic Search</h2>

<p>I’ve been working on a website for <a href="https://stampy.ai/wiki/Stampy">stampy.ai</a>, the community generated questions and answers about AGI Safety. At the moment, the database contains several hundred questions, but the goal is to eventually grow to be thousands or more.</p>

<p>Since we had access to OpenAI’s very powerful GPT-3, the original plan was to use its semantic search capabilities. Instead of only matching keywords, semantic search allows users to ask questions and search for already answered questions that are semantically similar or essentially mean the same thing. In order accomplish that, questions are converted in sentence embeddings or vectors which can then be compared using cosine similarity. If the vectors are already normalized, cosine similarity is calculated by a quick matrix multiplication. Due to the cost and the large files associated with embedding all of our questions, we realized we’d need to limit how often we made calls to OpenAI’s API and only submit requests once the user has completed typing their question.</p>

<h2 id="tensorflowjs-in-the-browser">Tensorflow.js in the Browser</h2>

<p>Having used features like Google search, we’re familiar with its ability to dynamically update the list of related search options as the user types. Since we’re not Google, I hadn’t dreamt that we could do then same. However, in my free time, I’m quite a tutorial junkie. I ran across a tutorial to create your own <a href="https://codelabs.developers.google.com/tensorflowjs-transfer-learning-teachable-machine#0">Teachable Machine</a> and a number of other demos with <a href="https://tensorflow.github.io/tfjs/">tensorflow.js (TFJS)</a> allowing models to make AI predictions in the browser. I suddenly realized that if it’s possible to train a model to identify objects from the desktop video camera on a browser in realtime, we must be able to run our semantic search in the browser, dynamically updating results as the user types in the search bar.</p>

<h2 id="universal-sentence-encoder">Universal Sentence Encoder</h2>

<p>To keep things simple, I started with a pre-trained model that was already optimized for web use. I found a promising model called the <a href="https://github.com/tensorflow/tfjs-models/tree/master/universal-sentence-encoder">Universal Sentence Encoder (USE)</a> which uses a 512-dimension vector to embed the sentences in contrast to GPT-3 which uses 12,288-dimensions. As a result the embeddings file for USE was only around 1.5MB compared to GPT-3’s embeddings which was over 150MB. That was for OpenAI’s largest Davinci model; going with a lighter model would lessen the gap but we’d still have expense issues. We obviously sacrificed some accuracy but the responsiveness really enhanced the user experience. To use the TFJS and USE libraries, we first imported the following scripts within the HTML:</p>

<div class="language-html highlighter-rouge"><div class="highlight"><pre class="highlight"><code>    <span class="nt">&lt;script </span><span class="na">src=</span><span class="s">"https://cdn.jsdelivr.net/npm/@tensorflow/tfjs"</span> <span class="na">type=</span><span class="s">"text/javascript"</span><span class="nt">/&gt;</span>
    <span class="o">&lt;</span><span class="nx">script</span> <span class="nx">src</span><span class="o">=</span><span class="dl">"</span><span class="s2">https://cdn.jsdelivr.net/npm/@tensorflow-models/universal-sentence-encoder</span><span class="dl">"</span><span class="o">/&gt;</span>
</code></pre></div></div>

<p>To begin, make sure both libraries and the model have been loaded. Then, you’re ready to create sentence embeddings for your questions. The USE stores embeddings as a 2D tensors with 512 dimensions or 512 separate values. Think of the embeddings as a numerical representation of a sentence or (in our case a question) where each dimension represents some feature or meaningful information about the sentence.</p>

<div class="language-javascript highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">// load Tensorflow's universal sentence encoder model</span>
<span class="kd">const</span> <span class="nx">langModel</span> <span class="o">=</span> <span class="k">await</span> <span class="nx">use</span><span class="p">.</span><span class="nx">load</span><span class="p">()</span>
<span class="c1">// embed list of all questions in database for later search</span>
<span class="kd">const</span> <span class="nx">questions</span> <span class="o">=</span> <span class="p">[</span><span class="dl">"</span><span class="s2">What is your first question?</span><span class="dl">"</span><span class="p">,</span> <span class="dl">"</span><span class="s2">Ask another question?</span><span class="dl">"</span><span class="p">]</span>
<span class="kd">const</span> <span class="nx">allEncodings</span> <span class="o">=</span> <span class="p">(</span><span class="k">await</span> <span class="nx">model</span><span class="p">.</span><span class="nx">embed</span><span class="p">(</span><span class="nx">questions</span><span class="p">)).</span><span class="nx">arraySync</span><span class="p">()</span>
</code></pre></div></div>

<p>We had a few hundred questions, so on my desktop, embeddings all the questions only took a few seconds. But on laptops and other machines, the initial embedding of all the questions took several minutes, which was intolerable. So I saved the embeddings so it was just a matter of loading them. Soon, things were working smoothly, even on mobile phones.</p>

<p>Now we’re ready to process the query when a user ask a new question. First, we must create embeddings for that new question in order to find the nearest embedding from within our existing list of all questions.</p>

<div class="language-javascript highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kd">const</span> <span class="nx">runSemanticSearch</span> <span class="o">=</span> <span class="p">(</span><span class="nx">searchQuery</span><span class="p">)</span> <span class="o">=&gt;</span> <span class="p">{</span>

  <span class="nx">encoding</span> <span class="o">=</span> <span class="k">await</span> <span class="nx">langModel</span><span class="p">.</span><span class="nx">embed</span><span class="p">(</span><span class="nx">searchQuery</span><span class="p">)</span>

  <span class="c1">// numerator of cosine similar is dot prod since vectors are normalized</span>
  <span class="kd">const</span> <span class="nx">scores</span> <span class="o">=</span> <span class="nx">tf</span><span class="p">.</span><span class="nx">matMul</span><span class="p">(</span><span class="nx">encoding</span><span class="p">,</span> <span class="nx">allEncodings</span><span class="p">,</span> <span class="kc">false</span><span class="p">,</span> <span class="kc">true</span><span class="p">).</span><span class="nx">dataSync</span><span class="p">()</span>

  <span class="c1">// Tensorflow requires explicit memory management to avoid memory leaks</span>
  <span class="nx">encoding</span><span class="p">.</span><span class="nx">dispose</span><span class="p">()</span>

  <span class="c1">// sort by scores then return top 5 results</span>
  <span class="kd">const</span> <span class="nx">questionsScored</span> <span class="o">=</span> <span class="nx">questions</span><span class="p">.</span><span class="nx">map</span><span class="p">((</span><span class="nx">question</span><span class="p">,</span> <span class="nx">index</span><span class="p">)</span> <span class="o">=&gt;</span> <span class="p">({</span> <span class="nx">question</span><span class="p">,</span> <span class="nx">score</span> <span class="p">}))</span>
  <span class="nx">questionsScored</span><span class="p">.</span><span class="nx">sort</span><span class="p">((</span><span class="nx">a</span><span class="p">,</span><span class="nx">b</span><span class="p">)</span>  <span class="o">=&gt;</span> <span class="nx">b</span><span class="p">.</span><span class="nx">score</span> <span class="o">-</span> <span class="nx">a</span><span class="p">.</span><span class="nx">score</span><span class="p">)</span>
  <span class="kd">const</span> <span class="nx">searchResults</span> <span class="o">=</span> <span class="nx">questionsScored</span><span class="p">.</span><span class="nx">slice</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>

<span class="p">}</span>
</code></pre></div></div>

<p>The USE also has a QnA model for question-answering. I tried the embedding all of the database answers to see if it would help with the search. Unfortunately, I didn’t see a marked improvement, so I’ll need to play around with that a bit more.</p>

<h2 id="worker-for-gui-blocking">Worker for GUI Blocking</h2>

<p>As we were getting ready to launch the website, there were concerns about GUI blocking issues. The semantic search was constantly being called with each character being typed. That impacted the GUI rendering causing jitteriness or lag, obviously a yucky user experience.</p>

<p>After a bit of research, I discovered this to be a known issue and a major concern within the tensorflow.js community. Although the Javascript calls are marked as asynchronous, it isn’t truly multi-threaded. Fortunately, the solution using <a href="https://developer.mozilla.org/en-US/docs/Web/API/Web_Workers_API/Using_web_workers">Web Workers</a> isn’t too complicated. Essentially, we put all of our computationally intensive Tensorflow code in a separate file which runs in another thread, independently of the main GUI rendering JavaScript thread. The main thread and the separate Tensorflow thread communicate by passing messages using the <code class="language-plaintext highlighter-rouge">postMessage</code> method and responding to messages via the <code class="language-plaintext highlighter-rouge">onmessage</code> handler.</p>

<p>In the main thread, we first create a Worker. When user makes a search request, we’ll call <code class="language-plaintext highlighter-rouge">postMessage</code> to dispatch the Worker. Then, an event handler listens for <code class="language-plaintext highlighter-rouge">onmmesage</code> responses from the Worker to get the search results.</p>

<div class="language-javascript highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  <span class="c1">// create a Web Worker and add event listener for search results</span>
  <span class="k">if</span> <span class="p">(</span><span class="nb">self</span><span class="p">.</span><span class="nx">Worker</span> <span class="o">&amp;&amp;</span> <span class="o">!</span><span class="nx">tfWorkerRef</span><span class="p">.</span><span class="nx">current</span><span class="p">)</span> <span class="p">{</span>
    <span class="nx">tfWorkerRef</span><span class="p">.</span><span class="nx">current</span> <span class="o">=</span> <span class="k">new</span> <span class="nx">Worker</span><span class="p">(</span><span class="dl">'</span><span class="s1">/tfWorker.js</span><span class="dl">'</span><span class="p">)</span>
    <span class="nx">tfWorkerRef</span><span class="p">.</span><span class="nx">current</span><span class="p">.</span><span class="nx">addEventListener</span><span class="p">(</span><span class="dl">'</span><span class="s1">message</span><span class="dl">'</span><span class="p">,</span> <span class="nx">handleWorker</span><span class="p">)</span>
  <span class="p">}</span> <span class="k">else</span> <span class="p">{</span>
    <span class="nx">console</span><span class="p">.</span><span class="nx">debug</span><span class="p">(</span><span class="dl">'</span><span class="s1">Sorry! No Web Worker support.</span><span class="dl">'</span><span class="p">)</span>
  <span class="p">}</span>

  <span class="c1">// postMessage to call semantic search in Worker thread passing user's search query</span>
  <span class="nx">console</span><span class="p">.</span><span class="nx">debug</span><span class="p">(</span><span class="dl">'</span><span class="s1">postMessage to tfWorker:</span><span class="dl">'</span><span class="p">,</span> <span class="nx">searchQuery</span><span class="p">)</span>
  <span class="nx">tfWorkerRef</span><span class="p">.</span><span class="nx">current</span><span class="p">?.</span><span class="nx">postMessage</span><span class="p">(</span><span class="nx">searchQuery</span><span class="p">)</span>

  <span class="c1">// listen for onmessage response from Worker thread with search results</span>
  <span class="kd">const</span> <span class="nx">handleWorker</span> <span class="o">=</span> <span class="p">(</span><span class="nx">event</span><span class="p">)</span> <span class="o">=&gt;</span> <span class="p">{</span>
    <span class="kd">const</span> <span class="p">{</span><span class="nx">data</span><span class="p">}</span> <span class="o">=</span> <span class="nx">event</span>
    <span class="nx">console</span><span class="p">.</span><span class="nx">debug</span><span class="p">(</span><span class="dl">'</span><span class="s1">onmessage from tfWorker:</span><span class="dl">'</span><span class="p">,</span> <span class="nx">data</span><span class="p">)</span>
    <span class="k">if</span> <span class="p">(</span><span class="nx">data</span><span class="p">.</span><span class="nx">searchResults</span><span class="p">)</span> <span class="p">{</span>
      <span class="nx">setSearchResults</span><span class="p">(</span><span class="nx">data</span><span class="p">.</span><span class="nx">searchResults</span><span class="p">)</span>
    <span class="p">}</span>
  <span class="p">}</span>
</code></pre></div></div>

<p>The <code class="language-plaintext highlighter-rouge">tfWorker.js</code> file must be stored in a public folder and written in plain JavaScript, not TypeScript. It will likewise need an <code class="language-plaintext highlighter-rouge">onmessage</code> event handler to listen for event calls from the main thread, run semantic search then return the search results by calling <code class="language-plaintext highlighter-rouge">postMessage</code>.</p>

<div class="language-javascript highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">// listening for message from main thread to call semantic search</span>
<span class="nb">self</span><span class="p">.</span><span class="nx">onmessage</span> <span class="o">=</span> <span class="p">(</span><span class="nx">e</span><span class="p">)</span> <span class="o">=&gt;</span> <span class="p">{</span>
  <span class="nx">runSemanticSearch</span><span class="p">(</span><span class="nx">e</span><span class="p">.</span><span class="nx">data</span><span class="p">)</span>
<span class="p">}</span>

<span class="kd">const</span> <span class="nx">runSemanticSearch</span> <span class="o">=</span> <span class="p">(</span><span class="nx">searchQuery</span><span class="p">)</span> <span class="o">=&gt;</span> <span class="p">{</span>
  <span class="p">...</span>
  <span class="c1">// instead of returning searchResults, send a postMessage back to main thread</span>
  <span class="nb">self</span><span class="p">.</span><span class="nx">postMessage</span><span class="p">({</span><span class="nx">searchResults</span><span class="p">})</span>
<span class="p">}</span>
</code></pre></div></div>

<p>That’s the general setup. In the meantime, feel free to play around with the <a href="https://square-pine-garment.glitch.me/">prototype demo</a> or take a peak at the <a href="https://github.com/ccstan99/stampy-tfjs">code</a>. There’s some extra code to load and save the embeddings. You can also see the deployed <a href="http://ui.stampy.ai/">Stampy website</a> and its <a href="https://github.com/StampyAI/stampy-ui">full implementation</a> using TypeScript, React, Remix, and Cloudflare. Although we ended up not using GPT-3 for the website’s semantic search, the Stampy chatbot still uses impressive and entertaining generative text capabilities.</p>

<h2 id="conclusion">Conclusion</h2>

<p>I’ve been studying machine learning for a while now, but it’s been mostly theory and isolated exercises. This was refreshing to finally get back to some hands on work, applying the technology in a practical setting. I was lucky to work on this project with smart people who set up the fundamental framework and gave me freedom to explore on my own. Eventually, I’d love to train my own model and tweak some other parameters to see if we can get better results. So many possibilities for continued tinkering and learning!</p>

<h2 id="references">References</h2>

<ul>
  <li><a href="https://arxiv.org/abs/1803.11175">Universal Sentence Encoder</a> (2018)</li>
  <li><a href="https://www.edx.org/course/google-ai-for-javascript-developers-with-tensorflowjs">Google AI for JavaScript developers with Tensorflow.js</a></li>
  <li><a href="https://www.tensorflow.org/hub/tutorials/semantic_similarity_with_tf_hub_universal_encoder">Semantic similarity tutorial</a></li>
</ul>]]></content><author><name>ccstan99</name></author><summary type="html"><![CDATA[Semantic Search]]></summary></entry><entry><title type="html">Technovation Challenge</title><link href="http://localhost:4000/2022/06/23/technovation.html" rel="alternate" type="text/html" title="Technovation Challenge" /><published>2022-06-23T00:00:00-07:00</published><updated>2022-06-23T00:00:00-07:00</updated><id>http://localhost:4000/2022/06/23/technovation</id><content type="html" xml:base="http://localhost:4000/2022/06/23/technovation.html"><![CDATA[<h2 id="technovation">Technovation</h2>

<p>Every year, <a href="https://www.technovation.org/">Technovation</a> challenges teams of girls around the world to learn and apply the skills needed to solve real-world problems for social good using technology. Adult judges and mentors aren’t required or expected to have a technical background beyond possessing curiosity and keeping an open mind to learn along with the students. A comprehensive training is provided during the onboarding and an easy to follow <a href="https://technovationchallenge.org/curriculum-intro/registered/new/">curriculum</a> guides you through every step. The lessons are broken down into 12 weeks but you can progress at your own pace. It’s truly satisfying to watch students work through the entire process from brainstorming ideas, designing &amp; building a prototype, writing a business plan, all the way to pitching the product.</p>

<p><img src="/img/posts/2022-06-23-technovation1.png" width="100%" /></p>

<h2 id="team-mentoring">Team Mentoring</h2>

<p>I was fortunate enough to mentor Team Code Work Ahead through the pandemic. Despite turbulent times, their enthusiasm and creativity was impressive. Initially, we met weekly, but during the onset &amp; confusion of the lockdown, the online meetings became daily while schools were still sorting themselves out. A detailed log of the entire <a href="https://sites.google.com/view/code-work-ahead/plan?authuser=0">planning processing</a> was kept. Ultimately, their hard work paid off as the <a href="https://sites.google.com/view/code-work-ahead/">Rooting for You</a> app was the North America Regional Winner in 2020!</p>

<p><img src="/img/posts/2022-06-23-technovation2.png" width="100%" /></p>

<h2 id="judging">Judging</h2>

<p>For the last few years, I’ve served as judge advisor to help evaluate submissions following a clear set of <a href="https://technovationchallenge.org/curriculum/judging-rubric/">rubrics</a>. It’s also a chance to offer encouraging feedback to all those inspirational ideas and amazing projects coming from all corners of the world. The annual World Summit is always an exciting culmination to celebrate all the hard work and accomplishments. Check out the next <a href="https://hopin.com/events/technovation-s-world-summit/registration">Summit</a> on August 12!</p>]]></content><author><name>ccstan99</name></author><summary type="html"><![CDATA[Technovation]]></summary></entry></feed>