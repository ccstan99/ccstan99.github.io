---
layout: post
title: "Find Duplicates with SBERT"
subtitle: "Sentence-BERT Paraphrase Mining"
background: '/img/posts/2022-06-14-sbert.jpg'
---

I've been working on a FAQ project where the questions culled from multiple sources including YouTube video comments and a Discord server. Those that are marked as high interest are then answered and evaluated by community of volunteers. At some point, we realized there's likely to be quite a number of duplicate or at least semantically very similar questions in the database. The concern is of course that this could be a potentially time-consuming and resource-heavy operation since we'd need to compare every question in the database with every other one giving us the dreaded O(n^2).

After a bit of research, I came across [SBERT.net](https://sbert.net/) and the sentence-transformers library. Apparently, large sites such as Quora deals with this exact issue. When users ask a new question,

fast + accurate results
q=10k, 50 million inference, 65 hours to seconds
Evergrowing number of [pretrained sentence-transformer models](https://sbert.net/docs/pretrained_models.html)
selected 3 best performances on STS

- `distilbert-base-nli-stsb-quora-ranking` trained on 500K Quora duplicate questions
- `multi-qa-mpnet-base-dot-v1` trained on 315M StackExchange, Yahoo Answers, Google & Bing questions, best semantic
- `all-MiniLM-L6-v2` general purpose model 1B+ training pairs, semantic search perf 49.54, fast and good (not great) quality
- `paraphrases-multi-qa-mpn` best results

```python
!pip install sentence-transformers

from sentence_transformers import SentenceTransformer, util

model_name = "multi-qa-mpnet-base-dot-v1"  
#@param ['distilbert-base-nli-stsb-quora-ranking', 'multi-qa-mpnet-base-dot-v1', 'all-MiniLM-L6-v2']
model = SentenceTransformer(model_name)

# Single list of sentences - Possible tens of thousands of sentences
sentences = stampy_df["fulltext"].values.tolist()

paraphrases = util.paraphrase_mining(model, sentences)

for paraphrase in paraphrases[0:100]:
    score, i, j = paraphrase
    print(f"{stampy_df['fulltext'][i]}\n{stampy_df['fulltext'][j]}\nscore:{score:.2f}\n")  
```

Here are the [slides](/docs/JournalClub%202022-07-27%20SBERT.pdf) from the presentation I gave on the Sentence-BERT journal paper, along with a sample [notebook](https://colab.research.google.com/github/ccstan99/ccstan99.github.io/blob/main/docs/sentence-transformer-paraphrase-mining.ipynb) that uses the sentence transformer's `paraphrase_mining` utility.

## References

- [Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks](https://arxiv.org/abs/1908.10084) (2019)
