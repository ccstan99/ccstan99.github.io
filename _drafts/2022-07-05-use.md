---
layout: post
title: "Semantic Search in Browser"
subtitle: "Universal Sentence Encoder with Tensorflow.js"
background: '/img/posts/2022-07-05-use.jpg'
---

I've been working on a website for community generated questions and answers about AGI Safety. At the moment, the database has several hundred questions, but the hope is to eventually grow to be thousands or more.

Since we had access to OpenAI's very powerful GPT-3, the plan was to use it semantic search. Instead of only matching keywords, semantic search would allow users would be able to ask questions and search for other questions that mean the same or are semantically similar.

Compute similarity using cosine similarity, matrix multiplication.

## Tensorflow.js in the Browser

I came across a demo and tutorial to create your own [Teachable Machine](https://codelabs.developers.google.com/tensorflowjs-transfer-learning-teachable-machine#0) and a number of other demos tensorflow.js running inference in the browser. If it's possible to train a model identify objects in realtime on a browser, I was curious what it would take to run our semantic search in the browser as the users types. Just to get things started, I wanted to use pre-trained model that was optimized for web used. The Universal Sentence Encoder seemed to fit the bill. We probably sacrificed some accuracy but the responsiveness is really cool.

```javascript
// initialize search properties
use.load().then(function (model) {
  langModel = model
  fetch('/assets/stampy-questions-encodings.json')
    .then((response) => response.json())
    .then((data) => {
      questions = data.questions
      encodings = tf.tensor2d(data.encodings)
      // successfully loaded model & downloaded encodings
      isReady = true
    })
})
```

```javascript
  // encodings is 2D tensor of 512-dims embeddings for each sentence
  langModel.embed(searchQuery).then((encoding) => {
    // numerator of cosine similar is dot prod since vectors normalized
    const scores = tf.matMul(encoding, encodings, false, true).dataSync()

    // tensorflow requires explicit memory management to avoid memory leaks
    encoding.dispose()

    const questionsScored = questions.map((title, index) => ({
      title,
      score: scores[index],
    }))
    questionsScored.sort(byScore)
    const searchResults = questionsScored.slice(0, numResults)
    const numQs = questions.length

    self.postMessage({searchResults, numQs})
  })
```

On my machine, the loading to a few seconds. But on laptops and other machines, the initial encoding of all the questions was taking minutes. That was unbearable. I saved the encodings so it was just a matter of loading them. Soon, things were working, even on mobile phones.

## Worker for GUI Blocking

As we were getting ready to launch the website, there was some concern about GUI blocking issues. As you type, the model was constantly conducting inference and that impacts the performance of the GUI rendering, so you see jitteriness or lag. That's generally perceived as an extremely yucky user experience.

After a little bit of research, it seems this is a known issue within the tensorflow.js community and obviously a major concern. Although the Javascript code is asynchronous, it isn't truly multi-threaded. Fortunately, the solution isn't too complicated... using Web Workers. Essentially, we put all of our intensive Tensorflow classes in a separate file which would run in a separate thread independently of the main JavaScript thread that renders the GUI. The main thread and Tensorflow thread communicates by passing messages with the `postMessage` method and responding to messages via the `onmessage` handler.

In the main thread:

```javascript
```

In the `worker.js` file:

```javascript
// listening for semantic search
self.onmessage = (e) => {
  runSemanticSearch(e.data)
}
```

```javascript
  if (!searchQueryRaw || attempt >= maxAttempts) {
    self.postMessage({searchResults: []})
    return
  }
```

## Conclusion

I've been studying machine learning for a while now, but it's been mostly theory and isolated exercises. So this was great to finally get back to some hands on experience using the technology in a practical setting. I was lucky to work on this project with smart people who got the fundamental framework in place and gave me freedom to explore on my own. Eventually, I hope to train my own model and tweak some other parameters to see if we can get better results. There's also a QnA model for question-answering. I tried the encoding all of the database answers to see if it would help with the search, but I didn't see a marked improvement, so I'll need to play around with that a bit more. So many possibilities to continue tinkering with and learning from!

In the meantime, feel free to play around with the [prototype demo](https://square-pine-garment.glitch.me/) or take a peak at the [code](https://github.com/ccstan99/stampy-tfjs). You can also see the [deployed website](http://ui.stampy.ai/) and its [full implementation](https://github.com/StampyAI/stampy-ui) using TypeScript, React, Remix, and Cloudflare.

## References

- [Google AI for JavaScript developers with Tensorflow.js](https://www.edx.org/course/google-ai-for-javascript-developers-with-tensorflowjs)
- [Semantic similarity tutorial](https://www.tensorflow.org/hub/tutorials/semantic_similarity_with_tf_hub_universal_encoder)
- [USE on tfhub](https://tfhub.dev/google/collections/universal-sentence-encoder/1)
- [USE paper](https://arxiv.org/abs/1803.11175) (2018)
