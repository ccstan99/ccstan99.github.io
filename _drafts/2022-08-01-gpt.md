---
layout: post
title: "GPT"
subtitle: "OpenAI's Generative Pre-trained Transformers"
background: '/img/posts/2022-08-01-gpt.jpg'
---

## Resources

- [The Journey of Open AI GPT models](https://medium.com/walmartglobaltech/the-journey-of-open-ai-gpt-models-32d95b7b7fb2)
- GPT-3: [Language Models are Few-Shot Learners](https://arxiv.org/abs/2005.14165)
- GPT-2: [Language Models are Unsupervised Multitask Learners](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf)
- GPT: [Improving Language Understanding by Generative Pre-Training](https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf)
- Codex: [Evaluating Large Language Models Trained on Code](https://arxiv.org/abs/2107.03374)
- DALL-E2: [Hierarchical Text-Conditional Image Generation with CLIP Latents](https://arxiv.org/abs/2204.06125)
- DALL-E: [Zero-Shot Text-to-Image Generation](https://arxiv.org/abs/2102.12092)
