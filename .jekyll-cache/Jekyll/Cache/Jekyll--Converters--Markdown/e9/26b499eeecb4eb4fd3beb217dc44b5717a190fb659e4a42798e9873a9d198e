I"€<p>Thanks to the incredible wealth of resources readily available on the internet, Iâ€™ve been catching up on the latest in Artificial Intelligence (AI), particularly Natural Language Processing (NLP). My hope is to focus on educational outreach, bringing STEM to underrepresented populations. Hereâ€™s some that were particularly helpful in my own learning.</p>

<h2 id="courses">Courses</h2>

<ul>
  <li>Coursera <a href="https://www.coursera.org/specializations/deep-learning">DeepLearning</a> specialization</li>
  <li>Coursera <a href="https://www.coursera.org/specializations/natural-language-processing">Natural Language Processing</a> specialization</li>
  <li>Stanford CS224n: <a href="http://web.stanford.edu/class/cs224n/">Natural Language Processing with Deep Learning</a></li>
  <li><a href="https://huggingface.co/course">HuggingFace Course</a></li>
  <li>Edx <a href="https://www.edx.org/course/google-ai-for-javascript-developers-with-tensorflowjs">Google AI for JavaScript developers with Tensorflow.js</a></li>
  <li><a href="https://github.com/ageron/handson-ml">Hands-on Machine Learning with Scikit-Learn and TensorFlow</a></li>
  <li>fast.ai <a href="https://course.fast.ai/">Practical Deep Learning for Coders</a></li>
  <li>Animated Math <a href="https://www.3blue1brown.com/">3Blue1Bown</a></li>
  <li>AGI Safety Fundamentals <a href="https://www.eacambridge.org/technical-alignment-curriculum">Technical Alignment Curriculum</a></li>
</ul>

<h2 id="nlp-general-overview">NLP General Overview</h2>

<ul>
  <li><a href="http://karpathy.github.io/2015/05/21/rnn-effectiveness/">The Unreasonable Effectiveness of Recurrent Neural Networks</a> (2015)</li>
  <li><a href="https://colah.github.io/posts/2015-08-Understanding-LSTMs/">Understanding LSTM Networks</a> (2015)</li>
  <li><a href="https://distill.pub/2016/augmented-rnns/">Attention and Augmented RNNs</a>(2016)</li>
  <li><a href="http://nlp.seas.harvard.edu/2018/04/03/attention.html">The Annotated Transformer</a> (2018)</li>
  <li><a href="https://jalammar.github.io/illustrated-transformer/">The Illustrated Transformer</a> (2018)</li>
  <li><a href="https://jalammar.github.io/illustrated-bert/">The Illustrated BERT, ELMo, and co.</a> (2021)</li>
</ul>

<h2 id="journal-papers">Journal Papers</h2>

<ul>
  <li>
    <p><a href="https://www.semanticscholar.org/paper/A-Neural-Probabilistic-Language-Model-Bengio-Ducharme/6c2b28f9354f667cd5bd07afc0471d8334430da7">A neural probabilistic language model</a></p>
  </li>
  <li>Word2Vec CBOW &amp; Skip-Gram: <a href="https://arxiv.org/abs/1301.3781">Efficient estimation of word representations in vector space</a> (2013)</li>
  <li>Word2Vec Negative Sampling: <a href="https://arxiv.org/abs/1310.4546">Distributed representation of words and phrases and their compositionality</a> (2013)</li>
  <li><a href="https://arxiv.org/abs/1502.03044">Show, attend and tell: Neural image caption generation with visual attention</a> (2015)</li>
  <li>Transformers: <a href="https://arxiv.org/abs/1706.03762">Attention is All you Need</a> (2017)</li>
</ul>

<p>My <a href="/2022/05/09/nlp-notes.html">notes</a> contain a compilation of links to other papers and definitions.</p>
:ET