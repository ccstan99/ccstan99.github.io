I"4
<p>I just finished the <a href="https://www.coursera.org/specializations/natural-language-processing">DeepLearning.ai’s NLP specialization</a> on Coursera. I also went through the <a href="http://web.stanford.edu/class/cs224n/">Stanford CS224n course in NLP</a> and read a bunch of journal articles. Sorting through alphabet soup was an undertaking in itself. Since I kept referring to my notes to compare features between the different language models and looking up benchmark datasets &amp; sources, I figured I’d plop the charts here in case they’re helpful for others.</p>

<h2 id="language-models">Language Models</h2>
<p>year/paper name/hf-link #L/A/dims/params
2018 GPT
2018 BERT
2019 GPT-2
2019 RoBERTa
2019 T5
2020 GPT-3</p>

<h2 id="acronyms">Acronyms</h2>
<p>NLP	natural language processing
NLU	natural language understanding
NLI	natural language inference
CoLA	Corpus of Linguistic Acceptability - grammatical sentence
SST	Stanford Sentiment Treebank - sentiment analysis, movie reviews
MRPC	Microsoft Research Paraphrase Corpus
STS-B	Semantic Textual Similarity benchmark - semantic similarity
QQP	Quora Question Pairs
MNLI	Multi-Genre NLI Corpus - textual (entailment contradiction, neutral), matched (in-domain)/mismatched (cross-domain)
QNLI	Question-answering NLI - SQuAd?
RTE	Recognizing Textual Entailment - news, Wikipedia
WNLI	Winograd NLI - reading comprehension, pronoun coreference resolution
DPR	Definite Pronoun Resolution - WSC
SemEval	Semantic Evaluation
FraCaS	A Framework for Computational Semantics
RACE	ReAding Comprehension from Examinations - question answering
LAMBADA	LAnguage Modeling Broadened to Account for Discourse Aspects - long range dependencies
CBT	Children’s Book Test
CoQA	Conversation Question Answering
SWAG	Situations with Adversarial Generation
WMT	Workshop in Machine Translation
UDA	unsupervised data augmentation
RL2	reinforcement learning fast &amp; slow
MAML	model-agnostic meta-learning
BPE	byte-pair encoding
C4	Colossal Clean Crawled Corpus based on Common Crawl based data
BiDAF	bidirection attention flow
CoVe	contextualized word vectors
HSIC	Hilbert-Schmidt Independence Criterion
PMI	Pointwise Mutual Information
WSD	word sense disambiguation
BoolQ	Boolean Questions
CB	CommitmentBank
CoPA	Choice of Plausible Alternatives
MultiRC	Multi-Sentence Reading Comprehension
ReCoRD	Reading Comprehension with Commonsense Reasoning Dataset
WiC	Word in Context
WSC	Winograd Schema Challenge
AFS	Argument Facet Similarity
TF-IDF	term frequency–inverse document frequency, reflect how important a word is to a document in a collection or corpus</p>
:ET