I"Ò,<p>I just finished the <a href="https://www.coursera.org/specializations/natural-language-processing">DeepLearning.aiâ€™s NLP specialization</a> on Coursera. I also went through the <a href="http://web.stanford.edu/class/cs224n/">Stanford CS224n course in NLP</a> and read a bunch of journal articles. Sorting through alphabet soup was an undertaking in itself. Since I kept referring to my notes to compare features between the different language models and looking up benchmark datasets &amp; sources, I figured Iâ€™d plop the charts here in case theyâ€™re helpful for others.</p>

<h2 id="language-models">Language Models</h2>

<table>
  <thead>
    <tr>
      <th style="text-align: center">year</th>
      <th style="text-align: left">model</th>
      <th style="text-align: left">description</th>
      <th>specs</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center">2013</td>
      <td style="text-align: left">word2vec</td>
      <td style="text-align: left"><a href="https://arxiv.org/abs/1301.3781">Word Representations in Vectors</a></td>
      <td>Â </td>
    </tr>
    <tr>
      <td style="text-align: center">2014</td>
      <td style="text-align: left">GloVe</td>
      <td style="text-align: left"><a href="https://www.semanticscholar.org/paper/GloVe%3A-Global-Vectors-for-Word-Representation-Pennington-Socher/f37e1b62a767a307c046404ca96bc140b3e68cb5">Global Vectors</a></td>
      <td>Â </td>
    </tr>
    <tr>
      <td style="text-align: center">2018</td>
      <td style="text-align: left">GPT</td>
      <td style="text-align: left"><a href="https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf">Generative Pre-trained Transformer</a></td>
      <td>Â </td>
    </tr>
    <tr>
      <td style="text-align: center">2018</td>
      <td style="text-align: left">BERT</td>
      <td style="text-align: left"><a href="https://arxiv.org/abs/1810.04805">Bidirectional Encoder Representation for Transformers</a></td>
      <td>Â </td>
    </tr>
    <tr>
      <td style="text-align: center">2019</td>
      <td style="text-align: left">GPT-2</td>
      <td style="text-align: left"><a href="https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf">Unsupervised Multitask Learning</a></td>
      <td>Â </td>
    </tr>
    <tr>
      <td style="text-align: center">2019</td>
      <td style="text-align: left">RoBERTa</td>
      <td style="text-align: left"><a href="https://arxiv.org/abs/1907.11692">Robustly Optimized BERT</a></td>
      <td>Â </td>
    </tr>
    <tr>
      <td style="text-align: center">2019</td>
      <td style="text-align: left">T5</td>
      <td style="text-align: left"><a href="https://arxiv.org/abs/1910.10683">Transfer Learning with Text-to-Text Transformer</a></td>
      <td>Â </td>
    </tr>
    <tr>
      <td style="text-align: center">2020</td>
      <td style="text-align: left">GPT-3</td>
      <td style="text-align: left"><a href="https://arxiv.org/abs/2005.14165">Few-Shot Learners</a></td>
      <td>Â </td>
    </tr>
  </tbody>
</table>

<h2 id="datasets">Datasets</h2>

<table>
  <thead>
    <tr>
      <th style="text-align: left">dataset</th>
      <th style="text-align: left">full name</th>
      <th style="text-align: left">description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: left">GLUE</td>
      <td style="text-align: left"><a href="https://huggingface.co/datasets/glue">General Language Understanding Evaluation Benchmark</a></td>
      <td style="text-align: left">Â </td>
    </tr>
    <tr>
      <td style="text-align: left">CoLA</td>
      <td style="text-align: left">Corpus of Linguistic Acceptability</td>
      <td style="text-align: left">Grammatically correct sentence?</td>
    </tr>
    <tr>
      <td style="text-align: left">SST</td>
      <td style="text-align: left">Stanford Sentiment Treebank</td>
      <td style="text-align: left">Sentiment analysis of movie reviews</td>
    </tr>
    <tr>
      <td style="text-align: left">MRPC</td>
      <td style="text-align: left">Microsoft Research Paraphrase Corpus</td>
      <td style="text-align: left">Sentence pairs semantically equivalent?</td>
    </tr>
    <tr>
      <td style="text-align: left">STS-b</td>
      <td style="text-align: left"><a href="https://arxiv.org/abs/1708.00055v1">Semantic Textual Similarity benchmark</a></td>
      <td style="text-align: left">Sentence pairs semantically equivalent?</td>
    </tr>
    <tr>
      <td style="text-align: left">QQP</td>
      <td style="text-align: left">Quora Question Pairs</td>
      <td style="text-align: left">Sentence pairs semantically equivalent?</td>
    </tr>
    <tr>
      <td style="text-align: left">MNLI</td>
      <td style="text-align: left">Multi-Genre NLI Corpus</td>
      <td style="text-align: left">Premise &amp; hypothesis textual (entailment contradiction, neutral), matched (in-domain)/mismatched (cross-domain)</td>
    </tr>
    <tr>
      <td style="text-align: left">QNLI</td>
      <td style="text-align: left">Question-answering NLI</td>
      <td style="text-align: left">Context sentence contains answer to question?</td>
    </tr>
    <tr>
      <td style="text-align: left">RTE</td>
      <td style="text-align: left">Recognizing Textual Entailment</td>
      <td style="text-align: left">Wikipedia/news sentence entails a given hypothesis?</td>
    </tr>
    <tr>
      <td style="text-align: left">WNLI</td>
      <td style="text-align: left">Winograd NLI</td>
      <td style="text-align: left">Reading comprehension, pronoun coreference resolution</td>
    </tr>
    <tr>
      <td style="text-align: left">DPR</td>
      <td style="text-align: left">Definite Pronoun Resolution</td>
      <td style="text-align: left">Â </td>
    </tr>
    <tr>
      <td style="text-align: left">SemEval</td>
      <td style="text-align: left">Semantic Evaluation</td>
      <td style="text-align: left">Â </td>
    </tr>
    <tr>
      <td style="text-align: left">FraCaS</td>
      <td style="text-align: left">Framework for Computational Semantics</td>
      <td style="text-align: left">Â </td>
    </tr>
    <tr>
      <td style="text-align: left">RACE</td>
      <td style="text-align: left"><a href="https://huggingface.co/datasets/race">ReAding Comprehension from Examinations</a></td>
      <td style="text-align: left">Question answering</td>
    </tr>
    <tr>
      <td style="text-align: left">LAMBADA</td>
      <td style="text-align: left">LAnguage Modeling Broadened to Account for Discourse Aspects</td>
      <td style="text-align: left">Long range dependencies</td>
    </tr>
    <tr>
      <td style="text-align: left">CBT</td>
      <td style="text-align: left"><a href="https://huggingface.co/datasets/cbt">Childrenâ€™s Book Test</a></td>
      <td style="text-align: left">Â </td>
    </tr>
    <tr>
      <td style="text-align: left">CoQA</td>
      <td style="text-align: left"><a href="https://huggingface.co/datasets/coqa">Conversation Question Answering</a></td>
      <td style="text-align: left">Â </td>
    </tr>
    <tr>
      <td style="text-align: left">SWAG</td>
      <td style="text-align: left">Situations with Adversarial Generation</td>
      <td style="text-align: left">Â </td>
    </tr>
    <tr>
      <td style="text-align: left">C4</td>
      <td style="text-align: left"><a href="https://huggingface.co/datasets/c4">Colossal Clean Crawled Corpus</a></td>
      <td style="text-align: left">Â </td>
    </tr>
    <tr>
      <td style="text-align: left">WSD</td>
      <td style="text-align: left">Word Sense Disambiguation</td>
      <td style="text-align: left">Â </td>
    </tr>
    <tr>
      <td style="text-align: left">BoolQ</td>
      <td style="text-align: left">Boolean Questions</td>
      <td style="text-align: left">Â </td>
    </tr>
    <tr>
      <td style="text-align: left">CB</td>
      <td style="text-align: left">CommitmentBank</td>
      <td style="text-align: left">Â </td>
    </tr>
    <tr>
      <td style="text-align: left">CoPA</td>
      <td style="text-align: left">Choice of Plausible Alternatives</td>
      <td style="text-align: left">Â </td>
    </tr>
    <tr>
      <td style="text-align: left">MultiRC</td>
      <td style="text-align: left">Multi-Sentence Reading Comprehension</td>
      <td style="text-align: left">Â </td>
    </tr>
    <tr>
      <td style="text-align: left">ReCoRD</td>
      <td style="text-align: left">Reading Comprehension with Commonsense Reasoning Dataset</td>
      <td style="text-align: left">Â </td>
    </tr>
    <tr>
      <td style="text-align: left">WiC</td>
      <td style="text-align: left">Word in Context</td>
      <td style="text-align: left">Â </td>
    </tr>
    <tr>
      <td style="text-align: left">WSC</td>
      <td style="text-align: left"><a href="http://commonsensereasoning.org/2011/papers/Levesque.pdf">Winograd Schema Challenge</a></td>
      <td style="text-align: left">Â </td>
    </tr>
    <tr>
      <td style="text-align: left">AFS</td>
      <td style="text-align: left">Argument Facet Similarity</td>
      <td style="text-align: left">Â </td>
    </tr>
  </tbody>
</table>

<h2 id="acronyms">Acronyms</h2>

<table>
  <thead>
    <tr>
      <th style="text-align: left">term</th>
      <th style="text-align: left">description</th>
      <th>Â </th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: left">NLP</td>
      <td style="text-align: left">Natural Language Processing</td>
      <td>Â </td>
    </tr>
    <tr>
      <td style="text-align: left">NLU</td>
      <td style="text-align: left">Natural Language Understanding</td>
      <td>Â </td>
    </tr>
    <tr>
      <td style="text-align: left">NLI</td>
      <td style="text-align: left">Natural Language Inference</td>
      <td>Â </td>
    </tr>
    <tr>
      <td style="text-align: left">BiDAF</td>
      <td style="text-align: left">Bidirection Attention Flow</td>
      <td>Â </td>
    </tr>
    <tr>
      <td style="text-align: left">BPE</td>
      <td style="text-align: left">Byte-Pair Encoding</td>
      <td>Â </td>
    </tr>
    <tr>
      <td style="text-align: left">TF-IDF</td>
      <td style="text-align: left">Term Frequencyâ€“Inverse Document Frequency</td>
      <td>Reflects how important a word is to a document in a collection or corpus</td>
    </tr>
    <tr>
      <td style="text-align: left">WMT</td>
      <td style="text-align: left">Workshop in Machine Translation</td>
      <td>Â </td>
    </tr>
    <tr>
      <td style="text-align: left">CoVe</td>
      <td style="text-align: left">Contextualized Word Vectors</td>
      <td>Â </td>
    </tr>
    <tr>
      <td style="text-align: left">HSIC</td>
      <td style="text-align: left">Hilbert-Schmidt Independence Criterion</td>
      <td>Â </td>
    </tr>
    <tr>
      <td style="text-align: left">PMI</td>
      <td style="text-align: left">Pointwise Mutual Information</td>
      <td>Â </td>
    </tr>
    <tr>
      <td style="text-align: left">UDA</td>
      <td style="text-align: left">Unsupervised Data Augmentation</td>
      <td>Â </td>
    </tr>
    <tr>
      <td style="text-align: left">RL2</td>
      <td style="text-align: left">Reinforcement Learning Fast &amp; Slow</td>
      <td>Â </td>
    </tr>
    <tr>
      <td style="text-align: left">MAML</td>
      <td style="text-align: left">Model-Agnostic Meta-Learning</td>
      <td>Â </td>
    </tr>
  </tbody>
</table>
:ET