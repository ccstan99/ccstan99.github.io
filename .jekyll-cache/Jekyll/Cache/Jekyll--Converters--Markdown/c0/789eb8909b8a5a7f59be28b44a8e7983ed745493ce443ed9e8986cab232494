I"<p>Thanks to the incredible wealth of resources readily available on the internet, I’ve been catching up on the latest in Artificial Intelligence (AI), particularly Natural Language Processing (NLP). My hope is to focus on educational outreach, bringing STEM to underrepresented populations. Here’s some I’ve found particularly helpful for my own learning.</p>

<h2 id="courses">Courses</h2>
<ul>
  <li>Coursera <a href="https://www.coursera.org/specializations/deep-learning">DeepLearning</a> specialization</li>
  <li>Coursera <a href="https://www.coursera.org/specializations/natural-language-processing">Natural Language Processing</a> specialization</li>
  <li>Stanford CS224n: <a href="http://web.stanford.edu/class/cs224n/">Natural Language Processing with Deep Learning</a></li>
  <li><a href="https://huggingface.co/course">HuggingFace Course</a></li>
  <li>Edx <a href="https://www.edx.org/course/google-ai-for-javascript-developers-with-tensorflowjs">Google AI for JavaScript developers with Tensorflow.js</a></li>
  <li><a href="https://github.com/ageron/handson-ml">Hands-on Machine Learning with Scikit-Learn and TensorFlow</a></li>
  <li>fast.ai <a href="https://course.fast.ai/">Practical Deep Learning for Coders</a></li>
  <li>Animated Math <a href="https://www.3blue1brown.com/">3Blue1Bown</a></li>
  <li>AGI Safety Fundamentals <a href="https://www.eacambridge.org/technical-alignment-curriculum">Technical Alignment Curriculum</a></li>
</ul>

<h2 id="nlp-general-overview">NLP General Overview</h2>
<ul>
  <li><a href="http://karpathy.github.io/2015/05/21/rnn-effectiveness/">The Unreasonable Effectiveness of Recurrent Neural Networks</a> (2015)</li>
  <li><a href="https://colah.github.io/posts/2015-08-Understanding-LSTMs/">Understanding LSTM Networks</a> (2015)</li>
  <li><a href="https://distill.pub/2016/augmented-rnns/">Attention and Augmented RNNs</a>(2016)</li>
  <li><a href="http://nlp.seas.harvard.edu/2018/04/03/attention.html">The Annotated Transformer</a> (2018)</li>
  <li><a href="https://jalammar.github.io/illustrated-transformer/">The Illustrated Transformer</a> (2018)</li>
  <li><a href="https://jalammar.github.io/illustrated-bert/">The Illustrated BERT, ELMo, and co.</a> (2021)</li>
</ul>

<h2 id="journal-papers-on-language-models">Journal Papers on Language Models</h2>
<ul>
  <li><a href="https://www.semanticscholar.org/paper/A-Neural-Probabilistic-Language-Model-Bengio-Ducharme/6c2b28f9354f667cd5bd07afc0471d8334430da7">A neural probabilistic language model</a> (2003)</li>
  <li>Word2Vec CBOW &amp; Skip-Gram: <a href="https://arxiv.org/abs/1301.3781">Efficient estimation of word representations in vector space</a> (2013)</li>
  <li>Word2Vec Negative Sampling: <a href="https://arxiv.org/abs/1310.4546">Distributed representation of words and phrases and their compositionality</a> (2013)</li>
  <li><a href="https://www.semanticscholar.org/paper/GloVe%3A-Global-Vectors-for-Word-Representation-Pennington-Socher/f37e1b62a767a307c046404ca96bc140b3e68cb5">GloVe: Global vectors for word representation</a> (2014)</li>
  <li><a href="https://arxiv.org/abs/1502.03044">Show, attend and tell: Neural image caption generation with visual attention</a> (2015)</li>
  <li>Transformers: <a href="https://arxiv.org/abs/1706.03762">Attention is All you Need</a> (2017)</li>
  <li>GPT: <a href="https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf">Improving Language Understanding by Generative Pre-Training</a> (2018)</li>
  <li>GPT-2: <a href="https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf">Language Models are unsupervised multitask learners</a> (2019)</li>
  <li>GPT-3: <a href="https://arxiv.org/abs/2005.14165">Language Models are Few-Shot Learners</a> (2020)</li>
  <li><a href="https://arxiv.org/abs/1810.04805">BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</a> (2019)</li>
  <li><a href="https://arxiv.org/abs/1907.11692">RoBERTa: A Robustly Optimized BERT Pretraining Approach</a> (2019)</li>
  <li>T5: <a href="https://arxiv.org/abs/1910.10683">Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer</a> (2019)</li>
  <li><a href="https://arxiv.org/abs/1910.13461">BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension</a> (2019)</li>
  <li>XLM: <a href="https://arxiv.org/abs/1901.07291">Cross-lingual Language Model Pretraining</a> (2019)</li>
</ul>

<h2 id="benchmarks--datasets">Benchmarks &amp; Datasets</h2>
<ul>
  <li><a href="https://www.semanticscholar.org/paper/Bleu%3A-a-Method-for-Automatic-Evaluation-of-Machine-Papineni-Roukos/d7da009f457917aa381619facfa5ffae9329a6e9">BLEU: A method for automatic evaluation of machine translation</a> (2002)</li>
  <li>GLUE: <a href="https://arxiv.org/abs/1804.07461v1">A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding</a> (2019)</li>
  <li><a href="https://arxiv.org/abs/1905.00537v2">SuperGLUE: A Stickier Benchmark for General-Purpose Language Understanding Systems</a> (2019)</li>
  <li><a href="https://arxiv.org/abs/1606.05250">SQuAD: 100,000+ Questions for Machine Comprehension of Text</a> (2016)</li>
  <li><a href="http://commonsensereasoning.org/2011/papers/Levesque.pdf">The Winograd Schema Challenge</a> (2011)</li>
  <li><a href="https://arxiv.org/abs/1907.10641">WinoGrande: An Adversarial Winograd Schema Challenge at Scale</a> (2019)</li>
</ul>

<h2 id="miscellaneous">Miscellaneous</h2>
<ul>
  <li><a href="https://arxiv.org/abs/1803.11175">Universal Sentence Encoder</a> (2018)</li>
  <li><a href="https://arxiv.org/abs/1908.10084">Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks</a> (2019)</li>
  <li><a href="https://arxiv.org/abs/2201.02387">The Defeat of the Winograd Schema Challenge</a> (2022)</li>
</ul>
:ET