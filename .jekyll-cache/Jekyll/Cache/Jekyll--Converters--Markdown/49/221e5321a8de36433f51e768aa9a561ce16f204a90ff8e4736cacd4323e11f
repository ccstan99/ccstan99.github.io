I"á <p>I just finished the <a href="https://www.coursera.org/specializations/natural-language-processing">DeepLearning.aiâ€™s NLP specialization</a> on Coursera. I also went through the <a href="http://web.stanford.edu/class/cs224n/">Stanford CS224n course in NLP</a> and read a bunch of journal articles. Sorting through alphabet soup was an undertaking in itself. Since I kept referring to my notes to compare features between the different language models and looking up benchmark datasets &amp; sources, I figured Iâ€™d plop the charts here in case theyâ€™re helpful for others.</p>

<h2 id="language-models">Language Models</h2>

<table>
  <thead>
    <tr>
      <th style="text-align: center">year</th>
      <th style="text-align: left">model</th>
      <th style="text-align: left">description</th>
      <th>specs</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center">2013</td>
      <td style="text-align: left">word2vec</td>
      <td style="text-align: left"><a href="https://arxiv.org/abs/1301.3781">Word Representations in Vectors</a></td>
      <td>Â </td>
    </tr>
    <tr>
      <td style="text-align: center">2014</td>
      <td style="text-align: left">GloVe</td>
      <td style="text-align: left"><a href="https://www.semanticscholar.org/paper/GloVe%3A-Global-Vectors-for-Word-Representation-Pennington-Socher/f37e1b62a767a307c046404ca96bc140b3e68cb5">Global Vectors</a></td>
      <td>Â </td>
    </tr>
    <tr>
      <td style="text-align: center">2018</td>
      <td style="text-align: left">GPT</td>
      <td style="text-align: left"><a href="https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf">Generative Pre-trained Transformer</a></td>
      <td>Â </td>
    </tr>
    <tr>
      <td style="text-align: center">2018</td>
      <td style="text-align: left">BERT</td>
      <td style="text-align: left"><a href="https://arxiv.org/abs/1810.04805">Bidirectional Encoder Representation for Transformers</a></td>
      <td>Â </td>
    </tr>
    <tr>
      <td style="text-align: center">2019</td>
      <td style="text-align: left">GPT-2</td>
      <td style="text-align: left"><a href="https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf">Unsupervised Multitask Learning</a></td>
      <td>Â </td>
    </tr>
    <tr>
      <td style="text-align: center">2019</td>
      <td style="text-align: left">RoBERTa</td>
      <td style="text-align: left"><a href="https://arxiv.org/abs/1907.11692">Robustly Optimized BERT</a></td>
      <td>Â </td>
    </tr>
    <tr>
      <td style="text-align: center">2019</td>
      <td style="text-align: left">T5</td>
      <td style="text-align: left"><a href="https://arxiv.org/abs/1910.10683">Transfer Learning with Text-to-Text Transformer</a></td>
      <td>Â </td>
    </tr>
    <tr>
      <td style="text-align: center">2020</td>
      <td style="text-align: left">GPT-3</td>
      <td style="text-align: left"><a href="https://arxiv.org/abs/2005.14165">Few-Shot Learners</a></td>
      <td>Â </td>
    </tr>
  </tbody>
</table>

<h2 id="datasets">Datasets</h2>

<p>| dataset | full name | description |
| :â€”â€” | :â€”â€”â€“ | :â€”â€”â€”- |
| GLUE | <a href="https://huggingface.co/datasets/glue">General Language Understanding Evaluation Benchmark</a>
| CoLA | <a href="https://huggingface.co/datasets/glue/viewer/cola/train">Corpus of Linguistic Acceptability</a> | Grammatically correct sentence?
| SST | <a href="https://huggingface.co/datasets/glue/viewer/sst2/train">Stanford Sentiment Treebank</a> | Sentiment analysis of movie reviews
| MRPC | <a href="https://huggingface.co/datasets/glue/viewer/mrpc/train">Microsoft Research Paraphrase Corpus</a> | Sentence pairs semantically equivalent?
| QQP | <a href="https://huggingface.co/datasets/glue/viewer/qqp/train">Quora Question Pairs</a> | Sentence pairs semantically equivalent?
| STS-b | <a href="https://arxiv.org/abs/1708.00055v1">Semantic Textual Similarity benchmark</a> | Sentence pairs semantically equivalent?
| QNLI |<a href="https://huggingface.co/datasets/glue/viewer/qnli/train">Question-answering NLI</a> | Context sentence contains answer to question?
(cross-domain)
| RTE | <a href="https://huggingface.co/datasets/glue/viewer/rte/train">Recognizing Textual Entailment</a> | Wikipedia/news sentence entails a given hypothesis?
| WNLI | <a href="https://huggingface.co/datasets/glue/viewer/wnli/train">Winograd NLI</a> | Reading comprehension, pronoun coreference resolution
| MNLI | <a href="https://huggingface.co/datasets/SetFit/mnli">Multi-Genre NLI Corpus</a> | Premise &amp; hypothesis textual (entailment contradiction, neutral), matched (in-domain)/mismatched 
| SuperGLUE | <a href="https://huggingface.co/datasets/super_glue">SuperGLUE</a>
| BoolQ | <a href="https://huggingface.co/datasets/boolq">Boolean Questions</a>
| CB | CommitmentBank
| CoPA | Choice of Plausible Alternatives
| DPR | Definite Pronoun Resolution
| SemEval | Semantic Evaluation
| FraCaS | Framework for Computational Semantics
| SQuAD | <a href="https://huggingface.co/datasets/squad">Stanford Question Answering Dataset</a> | Question answering
| RACE | <a href="https://huggingface.co/datasets/race">ReAding Comprehension from Examinations</a> | Question answering
| LAMBADA | <a href="https://huggingface.co/datasets/lambada">LAnguage Modeling Broadened to Account for Discourse Aspects</a> | Long range dependencies
| CBT | <a href="https://huggingface.co/datasets/cbt">Childrenâ€™s Book Test</a>
| CoQA | <a href="https://huggingface.co/datasets/coqa">Conversation Question Answering</a>
| SWAG | <a href="https://huggingface.co/datasets/swag">Situations with Adversarial Generation</a>
| C4 | <a href="https://huggingface.co/datasets/c4">Colossal Clean Crawled Corpus</a>
| WSD | Word Sense Disambiguation
| MultiRC | Multi-Sentence Reading Comprehension
| ReCoRD | Reading Comprehension with Commonsense Reasoning Dataset
| WiC | Word in Context
| WSC | <a href="http://commonsensereasoning.org/2011/papers/Levesque.pdf">Winograd Schema Challenge</a>
| AFS | Argument Facet Similarity</p>

<h2 id="acronyms">Acronyms</h2>

<table>
  <thead>
    <tr>
      <th style="text-align: left">term</th>
      <th style="text-align: left">description</th>
      <th>Â </th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: left">NLP</td>
      <td style="text-align: left">Natural Language Processing</td>
      <td>Â </td>
    </tr>
    <tr>
      <td style="text-align: left">NLU</td>
      <td style="text-align: left">Natural Language Understanding</td>
      <td>Â </td>
    </tr>
    <tr>
      <td style="text-align: left">NLI</td>
      <td style="text-align: left">Natural Language Inference</td>
      <td>Â </td>
    </tr>
    <tr>
      <td style="text-align: left">BiDAF</td>
      <td style="text-align: left">Bidirection Attention Flow</td>
      <td>Â </td>
    </tr>
    <tr>
      <td style="text-align: left">BPE</td>
      <td style="text-align: left">Byte-Pair Encoding</td>
      <td>Â </td>
    </tr>
    <tr>
      <td style="text-align: left">TF-IDF</td>
      <td style="text-align: left">Term Frequencyâ€“Inverse Document Frequency</td>
      <td>Reflects how important a word is to a document in a collection or corpus</td>
    </tr>
    <tr>
      <td style="text-align: left">WMT</td>
      <td style="text-align: left">Workshop in Machine Translation</td>
      <td>Â </td>
    </tr>
    <tr>
      <td style="text-align: left">CoVe</td>
      <td style="text-align: left">Contextualized Word Vectors</td>
      <td>Â </td>
    </tr>
    <tr>
      <td style="text-align: left">HSIC</td>
      <td style="text-align: left">Hilbert-Schmidt Independence Criterion</td>
      <td>Â </td>
    </tr>
    <tr>
      <td style="text-align: left">PMI</td>
      <td style="text-align: left">Pointwise Mutual Information</td>
      <td>Â </td>
    </tr>
    <tr>
      <td style="text-align: left">UDA</td>
      <td style="text-align: left">Unsupervised Data Augmentation</td>
      <td>Â </td>
    </tr>
    <tr>
      <td style="text-align: left">RL2</td>
      <td style="text-align: left">Reinforcement Learning Fast &amp; Slow</td>
      <td>Â </td>
    </tr>
    <tr>
      <td style="text-align: left">MAML</td>
      <td style="text-align: left">Model-Agnostic Meta-Learning</td>
      <td>Â </td>
    </tr>
  </tbody>
</table>
:ET