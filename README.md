# AI/ML Educational Resources

Welcome to my educational resources repo! I originally studied BA [Computational Linguistics at UCLA](https://linguistics.ucla.edu/) then MS Computer Science in [Human Computer Interaction (HCI) at Stanford](https://hci.stanford.edu/). I've worked as a software engineer and freelanced in UI/UX then took a long break to homeschool my kids before returning to my original fascination with AI, particularly Natural Language Processing (NLP). Currently, I focus on educational outreach, bringing STEM to underrepresented populations. Below are some helpful resources I've used for learning.

## Courses
- Coursera [DeepLearning](https://www.coursera.org/specializations/deep-learning) specialization 
- Coursera [Natural Language Processing](https://www.coursera.org/specializations/natural-language-processing) specialization
- Stanford CS224n: [Natural Language Processing with Deep Learning](http://web.stanford.edu/class/cs224n/)
- [HuggingFace Course](https://huggingface.co/course)
- Edx [Google AI for JavaScript developers with Tensorflow.js](https://www.edx.org/course/google-ai-for-javascript-developers-with-tensorflowjs)
- [Hands-on Machine Learning with Scikit-Learn and TensorFlow](https://github.com/ageron/handson-ml)
- fast.ai [Practical Deep Learning for Coders](https://course.fast.ai/)
- AGI Safety Fundamentals [Technical Alignment Curriculum](https://www.eacambridge.org/technical-alignment-curriculum)
- Animated Math [3Blue1Bown](https://www.3blue1brown.com/)

## NLP Readings
### General Overview
- [The Unreasonable Effectiveness of Recurrent Neural Networks](http://karpathy.github.io/2015/05/21/rnn-effectiveness/) (2015)
- [Understanding LSTM Networks](https://colah.github.io/posts/2015-08-Understanding-LSTMs/) (2015)
- [Attention and Augmented RNNs](https://distill.pub/2016/augmented-rnns/)(2016)
- [The Annotated Transformer](http://nlp.seas.harvard.edu/2018/04/03/attention.html) (2018)
- [The Illustrated Transformer](https://jalammar.github.io/illustrated-transformer/) (2018)
- [The Illustrated BERT, ELMo, and co.](http://jalammar.github.io/illustrated-bert/) (2021)

### Journal Papers on Language Models
- [A neural probabilistic language model](https://www.semanticscholar.org/paper/A-Neural-Probabilistic-Language-Model-Bengio-Ducharme/6c2b28f9354f667cd5bd07afc0471d8334430da7) (2003)
- Word2Vec CBOW & Skip-Gram: [Efficient estimation of word representations in vector space](https://arxiv.org/abs/1301.3781) (2013)
- Word2Vec Negative Sampling: [Distributed representation of words and phrases and their compositionality](https://arxiv.org/abs/1310.4546) (2013)	
- [GloVe: Global vectors for word representation](https://www.semanticscholar.org/paper/GloVe%3A-Global-Vectors-for-Word-Representation-Pennington-Socher/f37e1b62a767a307c046404ca96bc140b3e68cb5) (2014)
- [Show, attend and tell: Neural image caption generation with visual attention](https://arxiv.org/abs/1502.03044) (2015)
- Transformers: [Attention is All you Need](https://arxiv.org/abs/1706.03762) (2017)
- GPT: [Improving Language Understanding by Generative Pre-Training](https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf) (2018)
- GPT-2: [Language Models are unsupervised multitask learners](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf) (2019)
- GPT-3: [Language Models are Few-Shot Learners](https://arxiv.org/abs/2005.14165) (2020)
- [BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/abs/1810.04805) (2019)
- [RoBERTa: A Robustly Optimized BERT Pretraining Approach](https://arxiv.org/abs/1907.11692) (2019)
- T5: [Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer](https://arxiv.org/abs/1910.10683) (2019)
- [BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension](https://arxiv.org/abs/1910.13461) (2019)
- XLM: [Cross-lingual Language Model Pretraining](https://arxiv.org/abs/1901.07291) (2019)

### Benchmarks & Datasets
- [BLEU: A method for automatic evaluation of machine translation](https://www.semanticscholar.org/paper/Bleu%3A-a-Method-for-Automatic-Evaluation-of-Machine-Papineni-Roukos/d7da009f457917aa381619facfa5ffae9329a6e9) (2002)
- GLUE: [A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding](https://arxiv.org/abs/1804.07461v1) (2019)
- [SuperGLUE: A Stickier Benchmark for General-Purpose Language Understanding Systems](https://arxiv.org/abs/1905.00537v2) (2019)
- [SQuAD: 100,000+ Questions for Machine Comprehension of Text](https://arxiv.org/abs/1606.05250) (2016)
- [The Winograd Schema Challenge](http://commonsensereasoning.org/2011/papers/Levesque.pdf) (2011)
- [WinoGrande: An Adversarial Winograd Schema Challenge at Scale](https://arxiv.org/abs/1907.10641) (2019)

### Miscellaneous
- [Universal Sentence Encoder](https://arxiv.org/abs/1803.11175) (2018)
- [Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks](https://arxiv.org/abs/1908.10084) (2019)
- [The Defeat of the Winograd Schema Challenge](https://arxiv.org/abs/2201.02387) (2022)

## Organizations
For students interested in STEM, here are some great organizations I'm involved with
- [AI4ALL](https://ai-4-all.org/) - Summer portfolio project, Mentor
- [Technovation](https://www.technovation.org/) - Mobile app development for social good, Team Mentor & Judge Advisor
