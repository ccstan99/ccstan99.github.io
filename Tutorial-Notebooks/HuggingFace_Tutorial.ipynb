{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ab3QS8yJR2si"
      },
      "source": [
        "# AI4ALL - Summer Portfolio Project (Text Dataset)\n",
        "Adapted from the [Hugging Face tutorial](https://huggingface.co/course/chapter3/3?fw=tf), which provides an excellent introduction to the field of [Natural Language Processing (NLP)](https://huggingface.co/course/chapter1/2?fw=tf) and an overview to the large language models based on the [Transformer Architecture](https://huggingface.co/course/chapter1/4?fw=tf)\n",
        "\n",
        "To get started with this notebook, from the \"Runtime\" menu above, select \"Change runtime type\" to bring up \"Notebook settings.\" Be sure that \"Hardware accelerator\" is set to \"GPU.\" Run the following cells to install and run needed libraries for this notebook. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zZAav6U7R2sl"
      },
      "outputs": [],
      "source": [
        "!pip install datasets transformers[sentencepiece]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YBjo_uvx7WEG"
      },
      "outputs": [],
      "source": [
        "from datasets import Dataset, load_dataset\n",
        "from transformers import AutoTokenizer, DataCollatorWithPadding, TFAutoModelForSequenceClassification\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k7wW_z3euNtm"
      },
      "source": [
        "## Preprocess Dataset\n",
        "**Required changes**\n",
        "- Pass the name of your Hugging Face dataset as an argument to `load_dataset()`\n",
        "- Update the `tokenize_function()` to indicate your input text field\n",
        "- List your target `output_columns` as needed\n",
        "\n",
        "Optional tuning\n",
        "- Play with different values for `checkpoint` to see if accuracy improves\n",
        "- Look into [other models](https://huggingface.co/transformers/v3.0.2/model_doc/auto.html)\n",
        "- Adjust `batch_size` to see if processing time improves. Value should be a power of 2."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JhfTScGN9rI0"
      },
      "outputs": [],
      "source": [
        "dataset_name = \"imdb\" #@param [\"imdb\", \"amazon_us_reviews\"] {allow-input: true}\n",
        "checkpoint = \"bert-base-uncased\" #@param [\"bert-base-uncased\", \"bert-base-multilingual-uncased\", \"distilbert-base-uncased\", \"roberta-base\"] {allow-input: true}\n",
        "# \"facebook/bart-large-mnli\", \"distilgpt2\", \"EleutherAI/gpt-neox-20b\"\n",
        "# \"bert-base-multilingual-cased\", \"bert-base-multilingual-uncased\"\n",
        "# \"bert\", \"roberta\", \"distilbert\", \"openai-gpt\", \"gpt2\", \"transfo-xl\", \"t5\", \"xlnet\", \"xlm\", \"ctrl\"\n",
        "# \"bert-base-uncased\", \"bert-base-cased\", \"roberta-base\", \"roberta-large\", \"distilbert-base-uncased\"\n",
        "batch_size =  8#@param {type:\"integer\"}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uSXH19LIEb8R"
      },
      "source": [
        "### Preprocess [IMDB dataset](https://huggingface.co/datasets/imdb)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2QkaR0SK6Jp1"
      },
      "outputs": [],
      "source": [
        "# preprocess imdb dataset\n",
        "def format_data(split_name):\n",
        "  df = raw_datasets[split_name].to_pandas()\n",
        "  # language models expect column named 'labels'\n",
        "  df.rename(columns = {'label':'labels'}, inplace = True)\n",
        "  if split == 'test':\n",
        "    # divide test split between validation & test\n",
        "    #num_rows = df.shape[0] // 2\n",
        "    num_rows = 2500\n",
        "    raw_datasets['validation'] = Dataset.from_pandas(df[:num_rows])\n",
        "    raw_datasets[split_name] = Dataset.from_pandas(df[num_rows:])\n",
        "  else:\n",
        "    # keep train split as is\n",
        "    raw_datasets[split_name] = Dataset.from_pandas(df)\n",
        "  print(split_name,'\\n', df.head(5))\n",
        "\n",
        "if dataset_name == 'imdb':\n",
        "  raw_datasets = load_dataset('imdb')\n",
        "  splits = ['train','test']\n",
        "  for split in splits:\n",
        "    format_data(split)\n",
        "  # remove 'unsupervised' split to avoid tokenizing\n",
        "  raw_datasets.pop('unsupervised', None)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m4yNX2DTEm9r"
      },
      "source": [
        "### Preprocess [Amazon dataset](https://huggingface.co/datasets/amazon_us_reviews)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ECZA62V3F3EY"
      },
      "outputs": [],
      "source": [
        "# num_rows: 104975\n",
        "# input features = ['review_headline', 'review_body']\n",
        "# output target = ['star_rating']\n",
        "\n",
        "if dataset_name == 'amazon_us_reviews':\n",
        "  raw_datasets = load_dataset(dataset_name, \"Mobile_Electronics_v1_00\")\n",
        "  split_name = 'train'\n",
        "  df_raw = raw_datasets[split_name].to_pandas()\n",
        "\n",
        "  target_name = 'star_rating'\n",
        "  df_raw.rename(columns = {target_name:'labels'}, inplace = True)\n",
        "  df_raw['text'] = df_raw['review_headline'] + '\\n' + df_raw['review_body']\n",
        "\n",
        "  df = df_raw[['text', 'labels']]\n",
        "  # min = 1, max = 5, rescale to [0,4] for 5 labels\n",
        "  df[['labels']] = df[['labels']] - 1\n",
        "  num_rows = df.shape[0]\n",
        "  num_train = int(num_rows * .8)\n",
        "  num_valid = int(num_rows * .1)\n",
        "  '''\n",
        "  raw_datasets['train'] = Dataset.from_pandas(df[:num_train])\n",
        "  raw_datasets['validation'] = Dataset.from_pandas(df[num_train:-num_valid])\n",
        "  raw_datasets['test'] = Dataset.from_pandas(df[-num_valid:])\n",
        "  '''\n",
        "  raw_datasets['train'] = Dataset.from_pandas(df[:-4000])\n",
        "  raw_datasets['validation'] = Dataset.from_pandas(df[-4000:-2000])\n",
        "  raw_datasets['test'] = Dataset.from_pandas(df[-2000:])\n",
        "  \n",
        "  print(raw_datasets)\n",
        "  print(df.head(10))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cUST3w_2kIBh"
      },
      "source": [
        "### Preprocess your dataset\n",
        "Learn more about [processing data](https://huggingface.co/course/chapter3/2?fw=tf)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fGyuPBWwkPHB"
      },
      "outputs": [],
      "source": [
        "# add your code here\n",
        "def format_data(split_name):\n",
        "  df = raw_datasets[split_name].to_pandas()\n",
        "  # language models expect column named 'labels'\n",
        "  df.rename(columns = {'label':'labels'}, inplace = True)\n",
        "  raw_datasets[split_name] = Dataset.from_pandas(df)\n",
        "  print(split_name,'\\n', df.head(5))\n",
        "\n",
        "if dataset_name == 'your_dataset':\n",
        "  raw_datasets = load_dataset(dataset_name)\n",
        "  splits = ['train','validation','test']\n",
        "  for split in splits:\n",
        "    format_data(split)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RpRT8qhTFU38"
      },
      "outputs": [],
      "source": [
        "# if raw_dataset not yet defined, load it and hope for the best\n",
        "if not ('raw_datasets' in globals()):\n",
        "  raw_datasets = load_dataset(dataset_name) "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p5yQxRafLeBo"
      },
      "source": [
        "### Tokenize Text\n",
        "Learn more about [tokenizers](https://huggingface.co/course/chapter2/4?fw=tf)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wZ1mubnfR2sm"
      },
      "outputs": [],
      "source": [
        "### indicate text input field\n",
        "def tokenize_function(batch):\n",
        "    return tokenizer(batch[\"text\"], truncation=True)\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
        "tokenized_datasets = raw_datasets.map(tokenize_function, batched=True)\n",
        "data_collator = DataCollatorWithPadding(tokenizer=tokenizer, return_tensors=\"tf\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dryas7gKIpAq"
      },
      "outputs": [],
      "source": [
        "print(\"tokenized_datasets\", tokenized_datasets)\n",
        "for i in range(5):\n",
        "  print(f\"tokenized_datasets['validation'][{i}] {tokenized_datasets['validation'][i]}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "92A0uBAPIoS7"
      },
      "outputs": [],
      "source": [
        "# column input and target names the language model expects, do not rename\n",
        "input_columns = [\"attention_mask\", \"input_ids\", \"token_type_ids\"]\n",
        "label_column = [\"labels\"]\n",
        "\n",
        "tf_train_dataset = tokenized_datasets[\"train\"].to_tf_dataset(\n",
        "    columns=input_columns,\n",
        "    label_cols=label_column,\n",
        "    shuffle=True,\n",
        "    collate_fn=data_collator,\n",
        "    batch_size=batch_size,\n",
        ")\n",
        "\n",
        "tf_validation_dataset = tokenized_datasets[\"validation\"].to_tf_dataset(\n",
        "    columns=input_columns,\n",
        "    label_cols=label_column,\n",
        "    shuffle=False,\n",
        "    collate_fn=data_collator,\n",
        "    batch_size=batch_size,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8QC4Y4GJvqKV"
      },
      "source": [
        "## Load & Train Model\n",
        "**Required changes:**\n",
        "- If there's more than 1 possible label (not binary), set `num_labels` to the number of different labels\n",
        "- If each instance can include multiple labels, add a parameter `problem_type=\"multi_label_classification\"`\n",
        "- Select a `loss` function\n",
        "  - `BinaryCrossentropy` for binary classification (num_labels == 1)\n",
        "  - `SparseCategoricalCrossentropy` for multiple labels (num_labels >= 2)\n",
        "  - `CategoricalCrossentropy` for one-hot encoding, multi-label classification "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ap6RVqj1CwZa"
      },
      "outputs": [],
      "source": [
        "num_labels = 2 #@param {type:\"integer\"}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "umCY_t_jR2sn"
      },
      "outputs": [],
      "source": [
        "### if each target can contain multiple classes (floats label), add problem_type\n",
        "#model = TFAutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=num_labels, problem_type=\"multi_label_classification\")\n",
        "model = TFAutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=num_labels)\n",
        "\n",
        "loss_function = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "#loss_function=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
        "#loss_function=tf.keras.losses.CategoricalCrossentropy(from_logits=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ItiyFW8w_uXP"
      },
      "source": [
        "### Hypertuning Parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rb9kYFWd_m27"
      },
      "outputs": [],
      "source": [
        "num_epochs = 10#@param {type:\"integer\"}\n",
        "steps_per_epoch = 100#@param {type:\"integer\"}\n",
        "initial_learning_rate = 5e-5 #@param {type:\"number\"}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KxJF-VsU16nx"
      },
      "outputs": [],
      "source": [
        "# The number of training steps is the number of samples in the dataset, divided by the batch size then multiplied\n",
        "# by the total number of epochs. Note that the tf_train_dataset here is a batched tf.data.Dataset,\n",
        "# not the original Hugging Face Dataset, so its len() is already num_samples // batch_size.\n",
        "num_train_steps = len(tf_train_dataset) * num_epochs\n",
        "lr_scheduler = tf.keras.optimizers.schedules.PolynomialDecay(\n",
        "    initial_learning_rate=initial_learning_rate, end_learning_rate=0.0, decay_steps=num_train_steps\n",
        ")\n",
        "\n",
        "optimizer_function = tf.keras.optimizers.Adam(learning_rate=lr_scheduler)\n",
        "\n",
        "model.compile(\n",
        "    optimizer=optimizer_function,\n",
        "    loss=loss_function,\n",
        "    metrics=[\"accuracy\"],\n",
        ")\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e1yklr4MR2so"
      },
      "outputs": [],
      "source": [
        "print(f\"{dataset_name}, {checkpoint}, {batch_size}, {num_epochs}, {steps_per_epoch}, {initial_learning_rate}, {num_labels}\")\n",
        "\n",
        "history = model.fit(\n",
        "    tf_train_dataset, \n",
        "    validation_data=tf_validation_dataset, \n",
        "    epochs=num_epochs,\n",
        "    steps_per_epoch=steps_per_epoch,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "amR5aNDqLAx5"
      },
      "outputs": [],
      "source": [
        "print(f\"{dataset_name}, {checkpoint}, {batch_size}, {num_epochs}, {steps_per_epoch}, {initial_learning_rate}, {history.history['accuracy'][-1]}, {history.history['val_accuracy'][-1]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KCR9hhPX6mZD"
      },
      "source": [
        "## Evaluate Results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nPk-mqpM6Ffa"
      },
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mpimg\n",
        "acc = history.history['accuracy']\n",
        "val_acc = history.history['val_accuracy']\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "epochs=range(len(acc))\n",
        "plt.plot(epochs, acc, 'r', 'Training Accuracy')\n",
        "plt.plot(epochs, val_acc, 'b', 'Validation Accuracy')\n",
        "plt.title('Training and validation accuracy')\n",
        "plt.figure()\n",
        "plt.plot(epochs, loss, 'r', 'Training Loss')\n",
        "plt.plot(epochs, val_loss, 'b', 'Validation Loss')\n",
        "plt.title('Training and validation loss')\n",
        "plt.figure()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rExVCUThR2sp"
      },
      "outputs": [],
      "source": [
        "from datasets import load_metric\n",
        "\n",
        "tf_test_dataset = tokenized_datasets[\"test\"].to_tf_dataset(\n",
        "    columns=[\"attention_mask\", \"input_ids\", \"token_type_ids\"],\n",
        "    label_cols=[\"labels\"],\n",
        "    shuffle=False,\n",
        "    collate_fn=data_collator,\n",
        "    batch_size=8,\n",
        ")\n",
        "preds = model.predict(tf_test_dataset)[\"logits\"]\n",
        "class_preds = np.argmax(preds, axis=1)\n",
        "print(preds.shape, class_preds.shape)\n",
        "\n",
        "metric = load_metric(\"accuracy\")\n",
        "metric.compute(predictions=class_preds, references=raw_datasets[\"test\"][\"labels\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XhL4PmNQw6La"
      },
      "outputs": [],
      "source": [
        "!pip install colorama"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4wDBhxuYw64y"
      },
      "outputs": [],
      "source": [
        "from colorama import Fore, Style\n",
        "def print_results(pred, actual):\n",
        "  print(raw_datasets[\"test\"][i][\"text\"])\n",
        "  print(Fore.GREEN if pred == actual else Fore.RED , end =\"\")\n",
        "  if ('class_names' in globals()):\n",
        "    print(f'predicted: {pred} ({class_names[pred]}), actual: {actual} ({class_names[actual]})')\n",
        "  else:\n",
        "    print(f'predicted: {pred}, actual: {actual}\\n')\n",
        "  print(Style.RESET_ALL, end =\"\")  \n",
        "      \n",
        "# examine errors\n",
        "# for i in range(len(raw_datasets[\"test\"])):\n",
        "for i in range(50):\n",
        "  pred = class_preds[i]\n",
        "  actual = raw_datasets[\"test\"][i][\"labels\"]\n",
        "  #if (pred != actual):\n",
        "  print_results(pred, actual)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "54UUocXBcdn7"
      },
      "source": [
        "## Save & Test Model\n",
        "Save your fine-tuned model for later use."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lj6r8jSgLqyk"
      },
      "outputs": [],
      "source": [
        "saved_path = \"./my_tuned_model\" #@param {type:\"string\"}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "43iXQmlAcjDv"
      },
      "outputs": [],
      "source": [
        "model.save_pretrained(saved_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FvVaHXbJck0v"
      },
      "outputs": [],
      "source": [
        "reloaded_model = model.from_pretrained(saved_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q677ZNLfcnGS"
      },
      "outputs": [],
      "source": [
        "reloaded_preds = reloaded_model.predict(tf_test_dataset)[\"logits\"]\n",
        "reloaded_class_preds = np.argmax(reloaded_preds, axis=1)\n",
        "print(reloaded_preds.shape, class_preds.shape)\n",
        "\n",
        "metric = load_metric(\"accuracy\")\n",
        "metric.compute(predictions=reloaded_class_preds, references=raw_datasets[\"test\"][\"labels\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W7zu0zBv8nzS"
      },
      "source": [
        "Notebook created by [ChengCheng Tan](mailto:ccstan99@gmail.com). Feedback welcomed!"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "# AI4ALL - Summer Portfolio Project (Text Dataset) *TUTORIAL*",
      "private_outputs": true,
      "provenance": [],
      "toc_visible": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3.8.2 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.2"
    },
    "vscode": {
      "interpreter": {
        "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
